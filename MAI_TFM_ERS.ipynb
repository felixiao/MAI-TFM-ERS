{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSNAlIoncurZ"
      },
      "source": [
        "#TODOs:\n",
        "\n",
        "1. [ ] Config parser\n",
        "2. [ ] Using tensorflow\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuuEjsY4njO8"
      },
      "source": [
        "#Load Datas & Codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqAqZsBV_5vx",
        "outputId": "e8e6560d-d130-4633-e619-64f8ff16eb96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGXWQI3Gu0NW"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "# !rm -r ERS\n",
        "!git clone -l -s https://github.com/felixiao/MAI-TFM-ERS.git ERS/\n",
        "!rm ERS/MAI_TFM_ERS.ipynb\n",
        "!rm ERS/README.md\n",
        "!rm ERS/LICENSE\n",
        "\n",
        "!rm -r sample_data\n",
        "!unzip drive/MyDrive/Rec/PETER/Amazon.zip -d ./Data/\n",
        "!unzip drive/MyDrive/Rec/PETER/Yelp.zip -d ./Data/\n",
        "!unzip drive/MyDrive/Rec/PETER/TripAdvisor.zip -d ./Data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YneR6DQQPyHL"
      },
      "outputs": [],
      "source": [
        "!mkdir ./Result/\n",
        "!mkdir ./Result/Yelp/\n",
        "!mkdir ./Result/Yelp/PETER/\n",
        "!mkdir ./Result/Yelp/NRT/\n",
        "!mkdir ./Result/Yelp/NETE/\n",
        "!mkdir ./Result/Yelp/Att2Seq/\n",
        "!mkdir ./Result/Yelp/PEPLER/\n",
        "\n",
        "!mkdir ./Result/TripAdvisor/\n",
        "!mkdir ./Result/TripAdvisor/PETER/\n",
        "!mkdir ./Result/TripAdvisor/NRT/\n",
        "!mkdir ./Result/TripAdvisor/Att2Seq/\n",
        "!mkdir ./Result/TripAdvisor/NETE/\n",
        "!mkdir ./Result/TripAdvisor/PEPLER/\n",
        "\n",
        "!mkdir ./Result/Amazon/\n",
        "!mkdir ./Result/Amazon/ClothingShoesAndJewelry/\n",
        "!mkdir ./Result/Amazon/ClothingShoesAndJewelry/PETER/\n",
        "!mkdir ./Result/Amazon/ClothingShoesAndJewelry/NRT/\n",
        "!mkdir ./Result/Amazon/ClothingShoesAndJewelry/NETE/\n",
        "!mkdir ./Result/Amazon/ClothingShoesAndJewelry/PEPLER/\n",
        "!mkdir ./Result/Amazon/ClothingShoesAndJewelry/Att2Seq/\n",
        "\n",
        "!mkdir ./Result/Amazon/MoviesAndTV/\n",
        "!mkdir ./Result/Amazon/MoviesAndTV/PETER/\n",
        "!mkdir ./Result/Amazon/MoviesAndTV/NRT/\n",
        "!mkdir ./Result/Amazon/MoviesAndTV/NETE/\n",
        "!mkdir ./Result/Amazon/MoviesAndTV/PEPLER/\n",
        "!mkdir ./Result/Amazon/MoviesAndTV/Att2Seq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCp5yqf0PSnb"
      },
      "outputs": [],
      "source": [
        "!python -u ERS/PETER/main.py --cuda --peter_mask \\\n",
        "--log_interval 800 \\\n",
        "--data_path ./Data/Yelp/reviews.pickle \\\n",
        "--index_dir ./Data/Yelp/2/ \\\n",
        "--checkpoint ./Result/Yelp/PETER/2/ \\\n",
        "--use_feature >> ./Result/Yelp/PETER/Yelp_usefeat_2.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSR2OcfcSoMT"
      },
      "outputs": [],
      "source": [
        "!python -u ERS/PETER/main.py --cuda --peter_mask \\\n",
        "--log_interval 400 \\\n",
        "--data_path ./Data/MoviesAndTV/reviews.pickle \\\n",
        "--index_dir ./Data/MoviesAndTV/1/ \\\n",
        "--checkpoint ./Result/Amazon/MoviesAndTV/PETER/1/ \\\n",
        "--use_feature >> ./Result/Amazon/MoviesAndTV/PETER/Amz_MT_usefeat_1.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC45m7joAsH7"
      },
      "outputs": [],
      "source": [
        "!python -u ERS/PETER/main.py --cuda --peter_mask \\\n",
        "--log_interval 400 \\\n",
        "--data_path ./Data/MoviesAndTV/reviews.pickle \\\n",
        "--index_dir ./Data/MoviesAndTV/2/ \\\n",
        "--checkpoint ./Result/Amazon/MoviesAndTV/PETER/2/ \\\n",
        "--use_feature >> ./Result/Amazon/MoviesAndTV/PETER/Amz_MT_usefeat_2.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Eo1X79qWAsxa"
      },
      "outputs": [],
      "source": [
        "!python -u ERS/PETER/main.py --cuda --peter_mask \\\n",
        "--log_interval 400 \\\n",
        "--data_path ./Data/MoviesAndTV/reviews.pickle \\\n",
        "--index_dir ./Data/MoviesAndTV/3/ \\\n",
        "--checkpoint ./Result/Amazon/MoviesAndTV/PETER/3/ \\\n",
        "--use_feature >> ./Result/Amazon/MoviesAndTV/PETER/Amz_MT_usefeat_3.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FLNRMdhiAykd"
      },
      "outputs": [],
      "source": [
        "!python -u ERS/PETER/main.py --cuda --peter_mask \\\n",
        "--log_interval 400 \\\n",
        "--data_path ./Data/MoviesAndTV/reviews.pickle \\\n",
        "--index_dir ./Data/MoviesAndTV/4/ \\\n",
        "--checkpoint ./Result/Amazon/MoviesAndTV/PETER/4/ \\\n",
        "--use_feature >> ./Result/Amazon/MoviesAndTV/PETER/Amz_MT_usefeat_4.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joZ__PMHA08F"
      },
      "outputs": [],
      "source": [
        "!python -u ERS/PETER/main.py --cuda --peter_mask \\\n",
        "--log_interval 400 \\\n",
        "--data_path ./Data/MoviesAndTV/reviews.pickle \\\n",
        "--index_dir ./Data/MoviesAndTV/5/ \\\n",
        "--checkpoint ./Result/Amazon/MoviesAndTV/PETER/5/ \\\n",
        "--use_feature >> ./Result/Amazon/MoviesAndTV/PETER/Amz_MT_usefeat_5.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1MO-MnRoUw0",
        "outputId": "9b9f8afc-211c-449b-f63d-9cde341a0b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "from ERS.PETER.module import PETER\n",
        "from ERS.Utils.utils import rouge_score, bleu_score, DataLoader, Batchify, now_time, ids2tokens, unique_sentence_percent, \\\n",
        "    root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7oS_iraKy3C"
      },
      "source": [
        "# Setup Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoGyNgw1e5lH"
      },
      "outputs": [],
      "source": [
        "class Arg():\n",
        "  model_name = 'PETER' #@param [\"PETER\",\"NRT\"] {allow-input: true}\n",
        "  dataset_path = \"./Amazon/MoviesAndTV/\" #@param [\"./TripAdvisor/\", \"./Yelp/\", \"./Amazon/MoviesAndTV/\", \"./Amazon/ClothingShoesAndJewelry/\"] {allow-input: true}\n",
        "  index = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "  \n",
        "  data_path = os.path.join(dataset_path, \"reviews.pickle\")\n",
        "  index_dir = os.path.join(dataset_path, str(index))\n",
        "  checkpoint = os.path.join(\"./Result\",model_name, dataset_path[2:])\n",
        "  outf = \"generated.txt\" #@param {type:\"string\"}\n",
        "  \n",
        "  emsize = 512 #@param {type:\"integer\"}\n",
        "  nhead = 2 #@param {type:\"integer\"}\n",
        "  nhid = 2048 #@param {type:\"integer\"}\n",
        "  nlayers = 2 #@param {type:\"integer\"}\n",
        "  epochs = 10 #@param {type:\"slider\", min:10, max:200, step:1}\n",
        "  batch_size = 128 #@param {type:\"integer\"}\n",
        "  seed = 1111 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "  words = 15 #@param {type:\"slider\", min:12, max:20, step:1}\n",
        "  log_interval = 400 #@param {type:\"slider\", min:10, max:500, step:10}\n",
        "  vocab_size = 20000 #@param {type:\"integer\"}\n",
        "  endure_times = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "  l2_reg = 0 #@param{type:\"number\"}\n",
        "  rating_reg = 0.1 #@param {type:\"number\"}\n",
        "  context_reg = 1.0 #@param {type:\"number\"}\n",
        "  text_reg = 1.0 #@param {type:\"number\"}\n",
        "  dropout = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "  lr = 1.0 #@param {type:\"number\"}\n",
        "  clip = 1.0 #@param {type:\"number\"}\n",
        "  cuda = True #@param {type:\"boolean\"}\n",
        "  peter_mask = True #@param {type:\"boolean\"}\n",
        "  use_feature = True #@param {type:\"boolean\"}\n",
        "\n",
        "args = Arg()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGpwJUu_Jzkf"
      },
      "source": [
        "# PETER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkW033U0PXTD"
      },
      "outputs": [],
      "source": [
        "class PETER_Model():\n",
        "  def __init__(self,args):\n",
        "    # Set the random seed manually for reproducibility.\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "      if not args.cuda:\n",
        "        print(now_time() + 'WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
        "    self.device = torch.device('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "    if not os.path.exists(args.checkpoint):\n",
        "      os.makedirs(args.checkpoint)\n",
        "    self.model_path = os.path.join(args.checkpoint, 'model.pt')\n",
        "    self.prediction_path = os.path.join(args.checkpoint, args.outf)\n",
        "\n",
        "  def LoadData(self,args):\n",
        "    ###############################################################################\n",
        "    # Load data\n",
        "    ###############################################################################\n",
        "\n",
        "    print(now_time() + 'Loading data: ', args.data_path, args.index_dir)\n",
        "    self.corpus = DataLoader(args.data_path, args.index_dir, args.vocab_size,model=args.model_name)\n",
        "    self.word2idx = self.corpus.word_dict.word2idx\n",
        "    self.idx2word = self.corpus.word_dict.idx2word\n",
        "    self.feature_set = self.corpus.feature_set\n",
        "    self.train_data = Batchify(self.corpus.train, self.word2idx, args.words, args.batch_size, shuffle=True,model=args.model_name)\n",
        "    self.val_data = Batchify(self.corpus.valid, self.word2idx, args.words, args.batch_size,model=args.model_name)\n",
        "    self.test_data = Batchify(self.corpus.test, self.word2idx, args.words, args.batch_size,model=args.model_name)\n",
        "\n",
        "  def BuildModel(self,args):\n",
        "    ###############################################################################\n",
        "    # Build the model\n",
        "    ###############################################################################\n",
        "\n",
        "    if args.use_feature:\n",
        "        src_len = 2 + self.train_data.feature.size(1)  # [u, i, f]\n",
        "    else:\n",
        "        src_len = 2  # [u, i]\n",
        "    self.tgt_len = args.words + 1  # added <bos> or <eos>\n",
        "    self.ntokens = len(self.corpus.word_dict)\n",
        "    nuser = len(self.corpus.user_dict)\n",
        "    nitem = len(self.corpus.item_dict)\n",
        "    pad_idx = self.word2idx['<pad>']\n",
        "    self.model = PETER(args.peter_mask, src_len, self.tgt_len, pad_idx, nuser, nitem, self.ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(self.device)\n",
        "    self.text_criterion = nn.NLLLoss(ignore_index=pad_idx)  # ignore the padding when computing loss\n",
        "    self.rating_criterion = nn.MSELoss()\n",
        "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=args.lr)\n",
        "    self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=0.25)\n",
        "\n",
        "  def predict(self, log_context_dis, topk):\n",
        "    word_prob = log_context_dis.exp()  # (batch_size, ntoken)\n",
        "    if topk == 1:\n",
        "      context = torch.argmax(word_prob, dim=1, keepdim=True)  # (batch_size, 1)\n",
        "    else:\n",
        "      context = torch.topk(word_prob, topk, 1)[1]  # (batch_size, topk)\n",
        "    return context  # (batch_size, topk)\n",
        "\n",
        "  def train(self, args, data):\n",
        "    # Turn on training mode which enables dropout.\n",
        "    self.model.train()\n",
        "    context_loss = 0.\n",
        "    text_loss = 0.\n",
        "    rating_loss = 0.\n",
        "    total_sample = 0\n",
        "    while True:\n",
        "      user, item, rating, seq, feature = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "      batch_size = user.size(0)\n",
        "      user = user.to(self.device)  # (batch_size,)\n",
        "      item = item.to(self.device)\n",
        "      rating = rating.to(self.device)\n",
        "      seq = seq.t().to(self.device)  # (tgt_len + 1, batch_size)\n",
        "      feature = feature.t().to(self.device)  # (1, batch_size)\n",
        "      if args.use_feature:\n",
        "        text = torch.cat([feature, seq[:-1]], 0)  # (src_len + tgt_len - 2, batch_size)\n",
        "      else:\n",
        "        text = seq[:-1]  # (src_len + tgt_len - 2, batch_size)\n",
        "      # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "      # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "      self.optimizer.zero_grad()\n",
        "      log_word_prob, log_context_dis, rating_p, _ = self.model(user, item, text)  # (tgt_len, batch_size, ntoken) vs. (batch_size, ntoken) vs. (batch_size,)\n",
        "      context_dis = log_context_dis.unsqueeze(0).repeat((self.tgt_len - 1, 1, 1))  # (batch_size, ntoken) -> (tgt_len - 1, batch_size, ntoken)\n",
        "      c_loss = self.text_criterion(context_dis.view(-1, self.ntokens), seq[1:-1].reshape((-1,)))\n",
        "      r_loss = self.rating_criterion(rating_p, rating)\n",
        "      t_loss = self.text_criterion(log_word_prob.view(-1, self.ntokens), seq[1:].reshape((-1,)))\n",
        "      loss = args.text_reg * t_loss + args.context_reg * c_loss + args.rating_reg * r_loss\n",
        "      loss.backward()\n",
        "\n",
        "      # `clip_grad_norm` helps prevent the exploding gradient problem.\n",
        "      torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
        "      self.optimizer.step()\n",
        "\n",
        "      context_loss += batch_size * c_loss.item()\n",
        "      text_loss += batch_size * t_loss.item()\n",
        "      rating_loss += batch_size * r_loss.item()\n",
        "      total_sample += batch_size\n",
        "\n",
        "      if data.step % args.log_interval == 0 or data.step == data.total_step:\n",
        "        cur_c_loss = context_loss / total_sample\n",
        "        cur_t_loss = text_loss / total_sample\n",
        "        cur_r_loss = rating_loss / total_sample\n",
        "        print(now_time() + 'context ppl {:4.4f} | text ppl {:4.4f} | rating loss {:4.4f} | {:5d}/{:5d} batches'.format(\n",
        "              math.exp(cur_c_loss), math.exp(cur_t_loss), cur_r_loss, data.step, data.total_step))\n",
        "        context_loss = 0.\n",
        "        text_loss = 0.\n",
        "        rating_loss = 0.\n",
        "        total_sample = 0\n",
        "      if data.step == data.total_step:\n",
        "        break\n",
        "\n",
        "  def evaluate(self,args, data):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    self.model.eval()\n",
        "    context_loss = 0.\n",
        "    text_loss = 0.\n",
        "    rating_loss = 0.\n",
        "    total_sample = 0\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, rating, seq, feature = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "        batch_size = user.size(0)\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        rating = rating.to(self.device)\n",
        "        seq = seq.t().to(self.device)  # (tgt_len + 1, batch_size)\n",
        "        feature = feature.t().to(self.device)  # (1, batch_size)\n",
        "        if args.use_feature:\n",
        "          text = torch.cat([feature, seq[:-1]], 0)  # (src_len + tgt_len - 2, batch_size)\n",
        "        else:\n",
        "          text = seq[:-1]  # (src_len + tgt_len - 2, batch_size)\n",
        "        log_word_prob, log_context_dis, rating_p, _ = self.model(user, item, text)  # (tgt_len, batch_size, ntoken) vs. (batch_size, ntoken) vs. (batch_size,)\n",
        "        context_dis = log_context_dis.unsqueeze(0).repeat((self.tgt_len - 1, 1, 1))  # (batch_size, ntoken) -> (tgt_len - 1, batch_size, ntoken)\n",
        "        c_loss = self.text_criterion(context_dis.view(-1, self.ntokens), seq[1:-1].reshape((-1,)))\n",
        "        r_loss = self.rating_criterion(rating_p, rating)\n",
        "        t_loss = self.text_criterion(log_word_prob.view(-1, self.ntokens), seq[1:].reshape((-1,)))\n",
        "\n",
        "        context_loss += batch_size * c_loss.item()\n",
        "        text_loss += batch_size * t_loss.item()\n",
        "        rating_loss += batch_size * r_loss.item()\n",
        "        total_sample += batch_size\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return context_loss / total_sample, text_loss / total_sample, rating_loss / total_sample\n",
        "\n",
        "  def generate(self,args,data):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    self.model.eval()\n",
        "    idss_predict = []\n",
        "    context_predict = []\n",
        "    rating_predict = []\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, rating, seq, feature = data.next_batch()\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        bos = seq[:, 0].unsqueeze(0).to(self.device)  # (1, batch_size)\n",
        "        feature = feature.t().to(self.device)  # (1, batch_size)\n",
        "        if args.use_feature:\n",
        "          text = torch.cat([feature, bos], 0)  # (src_len - 1, batch_size)\n",
        "        else:\n",
        "          text = bos  # (src_len - 1, batch_size)\n",
        "        start_idx = text.size(0)\n",
        "        for idx in range(args.words):\n",
        "          # produce a word at each step\n",
        "          if idx == 0:\n",
        "            log_word_prob, log_context_dis, rating_p, _ = self.model(user, item, text, False)  # (batch_size, ntoken) vs. (batch_size, ntoken) vs. (batch_size,)\n",
        "            rating_predict.extend(rating_p.tolist())\n",
        "            context = self.predict(log_context_dis, topk=args.words)  # (batch_size, words)\n",
        "            context_predict.extend(context.tolist())\n",
        "          else:\n",
        "            log_word_prob, _, _, _ = self.model(user, item, text, False, False, False)  # (batch_size, ntoken)\n",
        "          word_prob = log_word_prob.exp()  # (batch_size, ntoken)\n",
        "          word_idx = torch.argmax(word_prob, dim=1)  # (batch_size,), pick the one with the largest probability\n",
        "          text = torch.cat([text, word_idx.unsqueeze(0)], 0)  # (len++, batch_size)\n",
        "        ids = text[start_idx:].t().tolist()  # (batch_size, seq_len)\n",
        "        idss_predict.extend(ids)\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "\n",
        "    # rating\n",
        "    predicted_rating = [(r, p) for (r, p) in zip(data.rating.tolist(), rating_predict)]\n",
        "    RMSE = root_mean_square_error(predicted_rating, self.corpus.max_rating, self.corpus.min_rating)\n",
        "    print(now_time() + 'RMSE {:7.4f}'.format(RMSE))\n",
        "    MAE = mean_absolute_error(predicted_rating, self.corpus.max_rating, self.corpus.min_rating)\n",
        "    print(now_time() + 'MAE {:7.4f}'.format(MAE))\n",
        "    # text\n",
        "    tokens_test = [ids2tokens(ids[1:], self.word2idx, self.idx2word) for ids in data.seq.tolist()]\n",
        "    tokens_predict = [ids2tokens(ids, self.word2idx, self.idx2word) for ids in idss_predict]\n",
        "    BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
        "    print(now_time() + 'BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "    BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
        "    print(now_time() + 'BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "    USR, USN = unique_sentence_percent(tokens_predict)\n",
        "    print(now_time() + 'USR {:7.4f} | USN {:7}'.format(USR, USN))\n",
        "    feature_batch = feature_detect(tokens_predict, self.feature_set)\n",
        "    DIV = feature_diversity(feature_batch)  # time-consuming\n",
        "    print(now_time() + 'DIV {:7.4f}'.format(DIV))\n",
        "    FCR = feature_coverage_ratio(feature_batch, self.feature_set)\n",
        "    print(now_time() + 'FCR {:7.4f}'.format(FCR))\n",
        "    feature_test = [self.idx2word[i] for i in data.feature.squeeze(1).tolist()]  # ids to words\n",
        "    FMR = feature_matching_ratio(feature_batch, feature_test)\n",
        "    print(now_time() + 'FMR {:7.4f}'.format(FMR))\n",
        "    text_test = [' '.join(tokens) for tokens in tokens_test]\n",
        "    text_predict = [' '.join(tokens) for tokens in tokens_predict]\n",
        "    tokens_context = [' '.join([self.idx2word[i] for i in ids]) for ids in context_predict]\n",
        "    ROUGE = rouge_score(text_test, text_predict)  # a dictionary\n",
        "    for (k, v) in ROUGE.items():\n",
        "      print(now_time() + '{} {:7.4f}'.format(k, v))\n",
        "    text_out = ''\n",
        "    for (real, ctx, fake) in zip(text_test, tokens_context, text_predict):\n",
        "      text_out += '{}\\n{}\\n{}\\n\\n'.format(real, ctx, fake)\n",
        "    return text_out\n",
        "\n",
        "  def TRAIN(self,args):\n",
        "      \n",
        "    # Loop over epochs.\n",
        "    best_val_loss = float('inf')\n",
        "    endure_count = 0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "      print(now_time() + 'epoch {}'.format(epoch))\n",
        "      self.train(args,self.train_data)\n",
        "      val_c_loss, val_t_loss, val_r_loss = self.evaluate(args,self.val_data)\n",
        "      if args.rating_reg == 0:\n",
        "        val_loss = val_t_loss\n",
        "      else:\n",
        "        val_loss = val_t_loss + val_r_loss\n",
        "      print(now_time() + 'context ppl {:4.4f} | text ppl {:4.4f} | rating loss {:4.4f} | valid loss {:4.4f} on validation'.format(\n",
        "        math.exp(val_c_loss), math.exp(val_t_loss), val_r_loss, val_loss))\n",
        "      # Save the model if the validation loss is the best we've seen so far.\n",
        "      if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        with open(self.model_path, 'wb') as f:\n",
        "          torch.save(self.model, f)\n",
        "      else:\n",
        "        endure_count += 1\n",
        "        print(now_time() + 'Endured {} time(s)'.format(endure_count))\n",
        "        if endure_count == args.endure_times:\n",
        "            print(now_time() + 'Cannot endure it anymore | Exiting from early stop')\n",
        "            break\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        self.scheduler.step()\n",
        "        print(now_time() + 'Learning rate set to {:2.8f}'.format(self.scheduler.get_last_lr()[0]))\n",
        "\n",
        "  def TEST(self,args):\n",
        "    # Load the best saved model.\n",
        "    with open(self.model_path, 'rb') as f:\n",
        "        self.model = torch.load(f).to(self.device)\n",
        "\n",
        "    # Run on test data.\n",
        "    test_c_loss, test_t_loss, test_r_loss = self.evaluate(args,self.test_data)\n",
        "    print('=' * 89)\n",
        "    print(now_time() + 'context ppl {:4.4f} | text ppl {:4.4f} | rating loss {:4.4f} on test | End of training'.format(\n",
        "        math.exp(test_c_loss), math.exp(test_t_loss), test_r_loss))\n",
        "\n",
        "    print(now_time() + 'Generating text')\n",
        "    text_o = self.generate(args,self.test_data)\n",
        "    with open(self.prediction_path, 'w', encoding='utf-8') as f:\n",
        "      f.write(text_o)\n",
        "    print(now_time() + 'Generated text saved to ({})'.format(self.prediction_path))\n",
        "\n",
        "    self.ShowGenSent(text_o,3)\n",
        "  \n",
        "  def ShowDataset(self,args,n=5):\n",
        "    with open(os.path.join(args.index_dir, 'validation.index'), 'r') as f:\n",
        "      valid_index = [int(x) for x in f.readline().split(' ')]\n",
        "      print('validation size: ',valid_index[0])\n",
        "      f.close()\n",
        "\n",
        "    reviews = pickle.load(open(args.data_path, 'rb'))\n",
        "    for i,review in enumerate(reviews[:n]):\n",
        "      print(i,review)\n",
        "      print('user:\\t',review['user'],'\\nitem:\\t',review['item'],'\\ntemp:\\t',review['template'],'\\nrating:\\t',review['rating'])\n",
        "      (fea, adj, tem, sco) = review['template']\n",
        "      print('fea:\\t',fea,'\\nadj:\\t', adj, '\\ntem:\\t',tem,'\\nsco:\\t', sco)\n",
        "\n",
        "  def ShowGenSent(self, gen_text,id):\n",
        "    texts = gen_text.split('\\n\\n')\n",
        "    print(id,'/',len(texts))\n",
        "    ts = texts[id].split('\\n')\n",
        "    print('GT : ',ts[0],'\\nCTX: ',ts[1],'\\nGEN: ',ts[2])\n",
        "    # print(text_test, tokens_context, text_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR8VHuk9bNZ7",
        "outputId": "5394dbae-1fdc-4671-ac69-48e25d0e11df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-11-02 22:23:39.002609]: Loading data:  ./Amazon/MoviesAndTV/reviews.pickle ./Amazon/MoviesAndTV/2\n",
            "[2022-11-02 22:23:52.885782]: epoch 1\n",
            "[2022-11-02 22:24:20.819452]: context ppl 47382.5616 | text ppl 45035.2354 | rating loss 13.6080 |   400/ 2762 batches\n",
            "[2022-11-02 22:24:49.352403]: context ppl 47463.1084 | text ppl 44949.0962 | rating loss 13.4653 |   800/ 2762 batches\n",
            "[2022-11-02 22:25:18.522203]: context ppl 47495.7530 | text ppl 45202.9664 | rating loss 12.7859 |  1200/ 2762 batches\n",
            "[2022-11-02 22:25:48.196921]: context ppl 47747.5363 | text ppl 44977.4790 | rating loss 11.4419 |  1600/ 2762 batches\n",
            "[2022-11-02 22:26:18.378987]: context ppl 47365.7255 | text ppl 44894.6036 | rating loss 11.1933 |  2000/ 2762 batches\n",
            "[2022-11-02 22:26:48.807786]: context ppl 47359.3785 | text ppl 45218.6345 | rating loss 10.8277 |  2400/ 2762 batches\n",
            "[2022-11-02 22:27:16.126388]: context ppl 47469.5643 | text ppl 45208.3515 | rating loss 11.1706 |  2762/ 2762 batches\n",
            "[2022-11-02 22:27:25.490293]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:27:25.906894]: epoch 2\n",
            "[2022-11-02 22:27:56.676636]: context ppl 47421.3536 | text ppl 45084.6954 | rating loss 12.0468 |   400/ 2762 batches\n",
            "[2022-11-02 22:28:27.171469]: context ppl 47421.7220 | text ppl 45070.9275 | rating loss 12.0622 |   800/ 2762 batches\n",
            "[2022-11-02 22:28:57.570600]: context ppl 47465.9774 | text ppl 45049.0325 | rating loss 12.1150 |  1200/ 2762 batches\n",
            "[2022-11-02 22:29:27.911111]: context ppl 47368.8186 | text ppl 45021.7694 | rating loss 12.0916 |  1600/ 2762 batches\n",
            "[2022-11-02 22:29:58.261002]: context ppl 47362.4307 | text ppl 44967.6745 | rating loss 12.1084 |  2000/ 2762 batches\n",
            "[2022-11-02 22:30:28.597492]: context ppl 47448.2521 | text ppl 44886.5517 | rating loss 12.0852 |  2400/ 2762 batches\n",
            "[2022-11-02 22:30:56.002090]: context ppl 47527.7296 | text ppl 45070.1969 | rating loss 12.0459 |  2762/ 2762 batches\n",
            "[2022-11-02 22:31:05.387628]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:31:05.387790]: Endured 1 time(s)\n",
            "[2022-11-02 22:31:05.389585]: Learning rate set to 0.00097656\n",
            "[2022-11-02 22:31:05.389633]: epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-11-02 22:31:36.037226]: context ppl 47326.8571 | text ppl 44997.0674 | rating loss 12.0830 |   400/ 2762 batches\n",
            "[2022-11-02 22:32:06.462627]: context ppl 47621.6900 | text ppl 45055.1683 | rating loss 12.0415 |   800/ 2762 batches\n",
            "[2022-11-02 22:32:36.920785]: context ppl 47473.9108 | text ppl 45055.4206 | rating loss 12.1151 |  1200/ 2762 batches\n",
            "[2022-11-02 22:33:07.385099]: context ppl 47416.6424 | text ppl 44933.5833 | rating loss 12.1282 |  1600/ 2762 batches\n",
            "[2022-11-02 22:33:37.882602]: context ppl 47468.7321 | text ppl 44947.4373 | rating loss 12.0592 |  2000/ 2762 batches\n",
            "[2022-11-02 22:34:08.344511]: context ppl 47387.7166 | text ppl 44980.1070 | rating loss 12.0610 |  2400/ 2762 batches\n",
            "[2022-11-02 22:34:35.854143]: context ppl 47375.6929 | text ppl 45041.3135 | rating loss 12.1004 |  2762/ 2762 batches\n",
            "[2022-11-02 22:34:45.271417]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:34:45.271567]: Endured 2 time(s)\n",
            "[2022-11-02 22:34:45.271664]: Learning rate set to 0.00024414\n",
            "[2022-11-02 22:34:45.271701]: epoch 4\n",
            "[2022-11-02 22:35:15.972749]: context ppl 47372.4072 | text ppl 45062.1005 | rating loss 12.1201 |   400/ 2762 batches\n",
            "[2022-11-02 22:35:46.434404]: context ppl 47296.8967 | text ppl 44938.4174 | rating loss 12.0703 |   800/ 2762 batches\n",
            "[2022-11-02 22:36:16.859883]: context ppl 47468.6570 | text ppl 44966.6207 | rating loss 12.1090 |  1200/ 2762 batches\n",
            "[2022-11-02 22:36:47.299537]: context ppl 47590.8548 | text ppl 45000.7884 | rating loss 12.0816 |  1600/ 2762 batches\n",
            "[2022-11-02 22:37:17.739491]: context ppl 47628.8738 | text ppl 45097.8464 | rating loss 12.0564 |  2000/ 2762 batches\n",
            "[2022-11-02 22:37:48.160075]: context ppl 47446.1445 | text ppl 45077.6076 | rating loss 12.0975 |  2400/ 2762 batches\n",
            "[2022-11-02 22:38:15.612738]: context ppl 47447.2691 | text ppl 44948.2905 | rating loss 12.0112 |  2762/ 2762 batches\n",
            "[2022-11-02 22:38:25.020440]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:38:25.020584]: Endured 3 time(s)\n",
            "[2022-11-02 22:38:25.020662]: Learning rate set to 0.00006104\n",
            "[2022-11-02 22:38:25.020690]: epoch 5\n",
            "[2022-11-02 22:38:55.643685]: context ppl 47451.8848 | text ppl 45162.9095 | rating loss 12.1195 |   400/ 2762 batches\n",
            "[2022-11-02 22:39:26.037769]: context ppl 47265.2063 | text ppl 44852.2777 | rating loss 12.0615 |   800/ 2762 batches\n",
            "[2022-11-02 22:39:56.450372]: context ppl 47502.7502 | text ppl 45145.6708 | rating loss 12.0800 |  1200/ 2762 batches\n",
            "[2022-11-02 22:40:26.852756]: context ppl 47497.7773 | text ppl 44915.9619 | rating loss 12.1267 |  1600/ 2762 batches\n",
            "[2022-11-02 22:40:57.286079]: context ppl 47490.7240 | text ppl 44990.6292 | rating loss 12.0848 |  2000/ 2762 batches\n",
            "[2022-11-02 22:41:27.761743]: context ppl 47401.9525 | text ppl 44973.4562 | rating loss 12.0730 |  2400/ 2762 batches\n",
            "[2022-11-02 22:41:55.264758]: context ppl 47410.0121 | text ppl 45107.1160 | rating loss 12.0599 |  2762/ 2762 batches\n",
            "[2022-11-02 22:42:04.706349]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:42:04.706483]: Endured 4 time(s)\n",
            "[2022-11-02 22:42:04.706575]: Learning rate set to 0.00001526\n",
            "[2022-11-02 22:42:04.706611]: epoch 6\n",
            "[2022-11-02 22:42:35.440266]: context ppl 47562.2786 | text ppl 44954.6498 | rating loss 12.0655 |   400/ 2762 batches\n",
            "[2022-11-02 22:43:05.943080]: context ppl 47498.9218 | text ppl 45106.4804 | rating loss 12.0732 |   800/ 2762 batches\n",
            "[2022-11-02 22:43:36.256477]: context ppl 47608.5823 | text ppl 45062.5175 | rating loss 12.1332 |  1200/ 2762 batches\n",
            "[2022-11-02 22:44:06.812121]: context ppl 47390.6365 | text ppl 44992.5954 | rating loss 12.0894 |  1600/ 2762 batches\n",
            "[2022-11-02 22:44:37.202843]: context ppl 47444.0166 | text ppl 45048.1502 | rating loss 12.0789 |  2000/ 2762 batches\n",
            "[2022-11-02 22:45:07.626314]: context ppl 47499.8290 | text ppl 45057.6437 | rating loss 12.0795 |  2400/ 2762 batches\n",
            "[2022-11-02 22:45:35.205022]: context ppl 47244.5593 | text ppl 44962.2615 | rating loss 12.0972 |  2762/ 2762 batches\n",
            "[2022-11-02 22:45:44.659649]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:45:44.659797]: Endured 5 time(s)\n",
            "[2022-11-02 22:45:44.659825]: Cannot endure it anymore | Exiting from early stop\n",
            "=========================================================================================\n",
            "[2022-11-02 22:45:54.242911]: context ppl 47929.8646 | text ppl 45132.3390 | rating loss 11.9484 on test | End of training\n",
            "[2022-11-02 22:45:54.242980]: Generating text\n",
            "[2022-11-02 22:46:35.005767]: RMSE  3.1772\n",
            "[2022-11-02 22:46:35.016534]: MAE  2.9480\n",
            "[2022-11-02 22:46:36.781412]: BLEU-1  0.0308\n",
            "[2022-11-02 22:46:41.396893]: BLEU-4  0.0000\n",
            "[2022-11-02 22:51:32.281715]: USR  0.7499 | USN   33132\n",
            "[2022-11-02 22:55:54.337686]: DIV  0.1351\n",
            "[2022-11-02 22:55:56.451318]: FCR  0.3521\n",
            "[2022-11-02 22:55:56.463233]: FMR  0.0006\n",
            "[2022-11-02 22:56:03.000991]: rouge_1/f_score  0.0349\n",
            "[2022-11-02 22:56:03.001148]: rouge_1/r_score  0.0431\n",
            "[2022-11-02 22:56:03.001190]: rouge_1/p_score  0.0314\n",
            "[2022-11-02 22:56:03.001224]: rouge_2/f_score  0.0000\n",
            "[2022-11-02 22:56:03.001257]: rouge_2/r_score  0.0000\n",
            "[2022-11-02 22:56:03.001288]: rouge_2/p_score  0.0000\n",
            "[2022-11-02 22:56:03.001321]: rouge_l/f_score  0.0318\n",
            "[2022-11-02 22:56:03.001353]: rouge_l/r_score  0.0413\n",
            "[2022-11-02 22:56:03.001386]: rouge_l/p_score  0.0306\n",
            "[2022-11-02 22:56:03.099830]: Generated text saved to (./Result/Amazon/MoviesAndTV/generated.txt)\n",
            "3 / 44180\n",
            "GT :  do n't waste your time on this loser . \n",
            "CTX:  enslaved genius publicly concentrates rereleased gump inexperienced extras saul never trio 29 expressive superbit winners \n",
            "GEN:  blackest communication clint larger details snob goblet crowd inarritu favourites hysterically belle footloose satisfies demeaning\n"
          ]
        }
      ],
      "source": [
        "peter = PETER_Model(args)\n",
        "peter.LoadData(args)\n",
        "peter.ShowDataset(args)\n",
        "peter.BuildModel(args)\n",
        "peter.TRAIN(args)\n",
        "peter.TEST(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GIRu2WEFzrX"
      },
      "source": [
        "#NRT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvHmLAK-F4Ns"
      },
      "outputs": [],
      "source": [
        "from ERS.NRT.module import NRT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caRjZ8AhfZKP"
      },
      "outputs": [],
      "source": [
        "class NRT_Model():\n",
        "  def __init__(self,args):\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "      if not args.cuda:\n",
        "        print(now_time() + 'WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
        "    self.device = torch.device('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "    if not os.path.exists(args.checkpoint):\n",
        "      os.makedirs(args.checkpoint)\n",
        "    self.model_path = os.path.join(args.checkpoint, 'model.pt')\n",
        "    self.prediction_path = os.path.join(args.checkpoint, args.outf)\n",
        "\n",
        "  def LoadData(self,args):\n",
        "    ###############################################################################\n",
        "    # Load data\n",
        "    ###############################################################################\n",
        "\n",
        "    print(now_time() + args.model_name,'Loading data: ', args.data_path, args.index_dir)\n",
        "    self.corpus = DataLoader(args.data_path, args.index_dir, args.vocab_size,model=args.model_name)\n",
        "    self.word2idx = self.corpus.word_dict.word2idx\n",
        "    self.idx2word = self.corpus.word_dict.idx2word\n",
        "    self.feature_set = self.corpus.feature_set\n",
        "    self.train_data = Batchify(self.corpus.train, self.word2idx, args.words, args.batch_size, shuffle=True,model=args.model_name)\n",
        "    self.val_data = Batchify(self.corpus.valid, self.word2idx, args.words, args.batch_size,model=args.model_name)\n",
        "    self.test_data = Batchify(self.corpus.test, self.word2idx, args.words, args.batch_size,model=args.model_name)\n",
        "\n",
        "  def BuildModel(self,args):\n",
        "    ###############################################################################\n",
        "    # Build the model\n",
        "    ###############################################################################\n",
        "\n",
        "    nuser = len(self.corpus.user_dict)\n",
        "    nitem = len(self.corpus.item_dict)\n",
        "    self.ntoken = len(self.corpus.word_dict)\n",
        "    pad_idx = self.word2idx['<pad>']\n",
        "    self.model = NRT(nuser, nitem, self.ntoken, args.emsize, args.nhid, args.nlayers, self.corpus.max_rating, self.corpus.min_rating).to(self.device)\n",
        "    self.text_criterion = nn.NLLLoss(ignore_index=pad_idx)  # ignore the padding when computing loss\n",
        "    self.rating_criterion = nn.MSELoss()\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)\n",
        "    #optimizer = torch.optim.Adadelta(model.parameters())  # lr is optional to Adadelta\n",
        "\n",
        "  \n",
        "  def train(self, atgs, data):\n",
        "    self.model.train()\n",
        "    text_loss = 0.\n",
        "    rating_loss = 0.\n",
        "    total_sample = 0\n",
        "    while True:\n",
        "      user, item, rating, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "      batch_size = user.size(0)\n",
        "      user = user.to(self.device)  # (batch_size,)\n",
        "      item = item.to(self.device)\n",
        "      rating = rating.to(self.device)\n",
        "      seq = seq.to(self.device)  # (batch_size, seq_len + 2)\n",
        "      # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "      # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "      self.optimizer.zero_grad()\n",
        "      rating_p, log_word_prob = self.model(user, item, seq[:, :-1])  # (batch_size,) vs. (batch_size, seq_len + 1, ntoken)\n",
        "      r_loss = self.rating_criterion(rating_p, rating)\n",
        "      t_loss = self.text_criterion(log_word_prob.view(-1, self.ntoken), seq[:, 1:].reshape((-1,)))\n",
        "      l2_loss = torch.cat([x.view(-1) for x in self.model.parameters()]).pow(2.).sum()\n",
        "      loss = args.text_reg * t_loss + args.rating_reg * r_loss + args.l2_reg * l2_loss\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      text_loss += batch_size * t_loss.item()\n",
        "      rating_loss += batch_size * r_loss.item()\n",
        "      total_sample += batch_size\n",
        "\n",
        "      if data.step == data.total_step:\n",
        "        break\n",
        "    return text_loss / total_sample, rating_loss / total_sample\n",
        "\n",
        "\n",
        "  def evaluate(self, args, data):\n",
        "    self.model.eval()\n",
        "    text_loss = 0.\n",
        "    rating_loss = 0.\n",
        "    total_sample = 0\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, rating, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "        batch_size = user.size(0)\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        rating = rating.to(self.device)\n",
        "        seq = seq.to(self.device)  # (batch_size, seq_len + 2)\n",
        "        rating_p, log_word_prob = self.model(user, item, seq[:, :-1])  # (batch_size,) vs. (batch_size, seq_len + 1, ntoken)\n",
        "        r_loss = self.rating_criterion(rating_p, rating)\n",
        "        t_loss = self.text_criterion(log_word_prob.view(-1, self.ntoken), seq[:, 1:].reshape((-1,)))\n",
        "\n",
        "        text_loss += batch_size * t_loss.item()\n",
        "        rating_loss += batch_size * r_loss.item()\n",
        "        total_sample += batch_size\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return text_loss / total_sample, rating_loss / total_sample\n",
        "\n",
        "\n",
        "  def generate(self, args, data):\n",
        "    self.model.eval()\n",
        "    idss_predict = []\n",
        "    rating_predict = []\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, _, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        inputs = seq[:, :1].to(self.device)  # (batch_size, 1)\n",
        "        hidden = None\n",
        "        ids = inputs\n",
        "        for idx in range(args.words):\n",
        "          # produce a word at each step\n",
        "          if idx == 0:\n",
        "            rating_p, hidden = self.model.encoder(user, item)\n",
        "            rating_predict.extend(rating_p.tolist())\n",
        "            log_word_prob, hidden = self.model.decoder(inputs, hidden)  # (batch_size, 1, ntoken)\n",
        "          else:\n",
        "            log_word_prob, hidden = self.model.decoder(inputs, hidden)  # (batch_size, 1, ntoken)\n",
        "          word_prob = log_word_prob.squeeze().exp()  # (batch_size, ntoken)\n",
        "          inputs = torch.argmax(word_prob, dim=1, keepdim=True)  # (batch_size, 1), pick the one with the largest probability\n",
        "          ids = torch.cat([ids, inputs], 1)  # (batch_size, len++)\n",
        "        ids = ids[:, 1:].tolist()  # remove bos\n",
        "        idss_predict.extend(ids)\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return idss_predict, rating_predict\n",
        "\n",
        "\n",
        "  def TRAIN(self,args):\n",
        "    # Loop over epochs.\n",
        "    best_val_loss = float('inf')\n",
        "    endure_count = 0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        print(now_time() + 'epoch {}'.format(epoch))\n",
        "        train_t_loss, train_r_loss = self.train(args,self.train_data)\n",
        "        print(now_time() + 'text ppl {:4.4f} | rating loss {:4.4f} | total loss {:4.4f} on train'.format(\n",
        "            math.exp(train_t_loss), train_r_loss, train_t_loss + train_r_loss))\n",
        "        val_t_loss, val_r_loss = self.evaluate(args,self.val_data)\n",
        "        val_loss = val_t_loss + val_r_loss\n",
        "        print(now_time() + 'text ppl {:4.4f} | rating loss {:4.4f} | total loss {:4.4f} on validation'.format(\n",
        "            math.exp(val_t_loss), val_r_loss, val_loss))\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            with open(self.model_path, 'wb') as f:\n",
        "                torch.save(self.model, f)\n",
        "        else:\n",
        "            endure_count += 1\n",
        "            print(now_time() + 'Endured {} time(s)'.format(endure_count))\n",
        "            if endure_count == args.endure_times:\n",
        "                print(now_time() + 'Cannot endure it anymore | Exiting from early stop')\n",
        "                break\n",
        "\n",
        "  def TEST(self,args):\n",
        "    # Load the best saved model.\n",
        "    with open(self.model_path, 'rb') as f:\n",
        "        self.model = torch.load(f).to(self.device)\n",
        "\n",
        "    # Run on test data.\n",
        "    test_t_loss, test_r_loss = self.evaluate(args,self.test_data)\n",
        "    print('=' * 89)\n",
        "    print(now_time() + 'text ppl {:4.4f} | rating loss {:4.4f} | total loss {:4.4f} on test | End of training'.format(\n",
        "            math.exp(test_t_loss), test_r_loss, test_t_loss + test_r_loss))\n",
        "    print(now_time() + args.model_name+ ' Generating text')\n",
        "    idss_predicted, rating_predicted = self.generate(args,self.test_data)\n",
        "    # rating\n",
        "    predicted_rating = [(r, p) for (r, p) in zip(self.test_data.rating.tolist(), rating_predicted)]\n",
        "    RMSE = root_mean_square_error(predicted_rating, self.corpus.max_rating, self.corpus.min_rating)\n",
        "    print(now_time() + 'RMSE {:7.4f}'.format(RMSE))\n",
        "    MAE = mean_absolute_error(predicted_rating, self.corpus.max_rating, self.corpus.min_rating)\n",
        "    print(now_time() + 'MAE {:7.4f}'.format(MAE))\n",
        "    # text\n",
        "    tokens_test = [ids2tokens(ids[1:], self.word2idx, self.idx2word) for ids in self.test_data.seq.tolist()]\n",
        "    tokens_predict = [ids2tokens(ids, self.word2idx, self.idx2word) for ids in idss_predicted]\n",
        "    BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
        "    print(now_time() + 'BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "    BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
        "    print(now_time() + 'BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "    USR, USN = unique_sentence_percent(tokens_predict)\n",
        "    print(now_time() + 'USR {:7.4f} | USN {:7}'.format(USR, USN))\n",
        "    feature_batch = feature_detect(tokens_predict, self.feature_set)\n",
        "    DIV = feature_diversity(feature_batch)  # time-consuming\n",
        "    print(now_time() + 'DIV {:7.4f}'.format(DIV))\n",
        "    FCR = feature_coverage_ratio(feature_batch, self.feature_set)\n",
        "    print(now_time() + 'FCR {:7.4f}'.format(FCR))\n",
        "    FMR = feature_matching_ratio(feature_batch, self.test_data.feature)\n",
        "    print(now_time() + 'FMR {:7.4f}'.format(FMR))\n",
        "    text_test = [' '.join(tokens) for tokens in tokens_test]\n",
        "    text_predict = [' '.join(tokens) for tokens in tokens_predict]\n",
        "    ROUGE = rouge_score(text_test, text_predict)  # a dictionary\n",
        "    for (k, v) in ROUGE.items():\n",
        "        print(now_time() + '{} {:7.4f}'.format(k, v))\n",
        "    text_out = ''\n",
        "    for (real, fake) in zip(text_test, text_predict):\n",
        "        text_out += '{}\\n{}\\n\\n'.format(real, fake)\n",
        "    with open(self.prediction_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(text_out)\n",
        "    print(now_time() + 'Generated text saved to ({})'.format(self.prediction_path))\n",
        "\n",
        "    self.ShowGenSent(text_out,3)\n",
        "\n",
        "  def ShowDataset(self,args,n=5):\n",
        "    with open(os.path.join(args.index_dir, 'validation.index'), 'r') as f:\n",
        "      valid_index = [int(x) for x in f.readline().split(' ')]\n",
        "      print('validation size: ',valid_index[0])\n",
        "      f.close()\n",
        "\n",
        "    reviews = pickle.load(open(args.data_path, 'rb'))\n",
        "    for i,review in enumerate(reviews[:n]):\n",
        "      print(i,review)\n",
        "      print('user:\\t',review['user'],'\\nitem:\\t',review['item'],'\\ntemp:\\t',review['template'],'\\nrating:\\t',review['rating'])\n",
        "      (fea, adj, tem, sco) = review['template']\n",
        "      print('fea:\\t',fea,'\\nadj:\\t', adj, '\\ntem:\\t',tem,'\\nsco:\\t', sco)\n",
        "\n",
        "  def ShowGenSent(self, gen_text,id):\n",
        "    texts = gen_text.split('\\n\\n')\n",
        "    print(id,'/',len(texts))\n",
        "    ts = texts[id].split('\\n')\n",
        "    print('GT : ',ts[0],'\\nCTX: ',ts[1],'\\nGEN: ',ts[2])\n",
        "    # print(text_test, tokens_context, text_predict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l94H02aEj2TA"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trf_nzbIj0f5"
      },
      "outputs": [],
      "source": [
        "args.model_name = 'NRT'\n",
        "args.epochs = 3\n",
        "args.emsize = 300\n",
        "args.nhid= 400\n",
        "args.nlayers = 4\n",
        "args.lr = 1e-4\n",
        "args.batch_size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "p-bqsR90kBtd",
        "outputId": "f780c3df-d158-4976-857b-5a220f0264c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[22-11-03 03:01:04.335247]: NRT Loading data:  ./Amazon/MoviesAndTV/reviews.pickle ./Amazon/MoviesAndTV/2\n",
            "[22-11-03 03:01:20.270448]: epoch 1\n",
            "[22-11-03 03:03:40.348605]: text ppl 419.8684 | rating loss 1.5290 | total loss 7.5689 on train\n",
            "[22-11-03 03:03:45.166604]: text ppl 241.9121 | rating loss 1.4150 | total loss 6.9036 on validation\n",
            "[22-11-03 03:03:45.384119]: epoch 2\n",
            "[22-11-03 03:06:05.545393]: text ppl 192.9768 | rating loss 1.4175 | total loss 6.6800 on train\n",
            "[22-11-03 03:06:10.466324]: text ppl 169.7756 | rating loss 1.4025 | total loss 6.5370 on validation\n",
            "[22-11-03 03:06:10.704998]: epoch 3\n",
            "[22-11-03 03:08:31.100325]: text ppl 150.2257 | rating loss 1.0881 | total loss 6.1003 on train\n",
            "[22-11-03 03:08:35.958335]: text ppl 142.2151 | rating loss 0.9388 | total loss 5.8962 on validation\n",
            "=========================================================================================\n",
            "[22-11-03 03:08:41.092057]: text ppl 141.1681 | rating loss 0.9588 | total loss 5.9088 on test | End of training\n",
            "[22-11-03 03:08:41.092147]: NRT Generating text\n",
            "[22-11-03 03:08:47.860756]: RMSE  0.9791\n",
            "[22-11-03 03:08:47.871371]: MAE  0.7254\n",
            "[22-11-03 03:08:49.322390]: BLEU-1  8.9448\n",
            "[22-11-03 03:08:52.571975]: BLEU-4  0.4735\n",
            "[22-11-03 03:08:52.643602]: USR  0.0007 | USN      31\n",
            "[22-11-03 03:14:26.248451]: DIV  3.9838\n",
            "[22-11-03 03:14:26.265914]: FCR  0.0021\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-465d60781846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-45dc724a0020>\u001b[0m in \u001b[0;36mTEST\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mFCR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_coverage_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FCR {:7.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFCR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mFMR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_matching_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FMR {:7.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFMR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mtext_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ERS/Utils/utils.py\u001b[0m in \u001b[0;36mfeature_matching_ratio\u001b[0;34m(feature_batch, test_feature)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfea_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfea_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "nrt = NRT_Model(args)\n",
        "nrt.LoadData(args)\n",
        "# nrt.ShowDataset(args)\n",
        "nrt.BuildModel(args)\n",
        "nrt.TRAIN(args)\n",
        "nrt.TEST(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoUYm0fi_SY_"
      },
      "source": [
        "#Att2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5DP6ic8_WHD"
      },
      "outputs": [],
      "source": [
        "from ERS.Att2Seq.module import Att2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuhphemzAGqv"
      },
      "outputs": [],
      "source": [
        "class Att2Seq_Model():\n",
        "  def __init__(self,args):\n",
        "    if torch.cuda.is_available():\n",
        "      if not args.cuda:\n",
        "        print(now_time() + 'WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
        "    self.device = torch.device('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "    if not os.path.exists(args.checkpoint):\n",
        "      os.makedirs(args.checkpoint)\n",
        "    self.model_path = os.path.join(args.checkpoint, 'model.pt')\n",
        "    self.prediction_path = os.path.join(args.checkpoint, args.outf)\n",
        "\n",
        "  def LoadData(self,args):\n",
        "    ###############################################################################\n",
        "    # Load data\n",
        "    ###############################################################################\n",
        "\n",
        "    print(now_time() +args.model_name+ ' Loading data ', args.data_path, args.index_dir)\n",
        "    self.corpus = DataLoader(args.data_path, args.index_dir, args.vocab_size,model=args.model_name)\n",
        "    self.word2idx = self.corpus.word_dict.word2idx\n",
        "    self.idx2word = self.corpus.word_dict.idx2word\n",
        "    self.feature_set = self.corpus.feature_set\n",
        "    self.train_data = Batchify(self.corpus.train, self.word2idx, args.words, args.batch_size, shuffle=True,model=args.model_name)\n",
        "    self.val_data = Batchify(self.corpus.valid, self.word2idx, args.words, args.batch_size,model=args.model_name)\n",
        "    self.test_data = Batchify(self.corpus.test, self.word2idx, args.words, args.batch_size,model=args.model_name)\n",
        "\n",
        "  def BuildModel(self,args):\n",
        "    ###############################################################################\n",
        "    # Build the model\n",
        "    ###############################################################################\n",
        "\n",
        "    nuser = len(self.corpus.user_dict)\n",
        "    nitem = len(self.corpus.item_dict)\n",
        "    self.ntoken = len(self.corpus.word_dict)\n",
        "    pad_idx = self.word2idx['<pad>']\n",
        "    self.model = Att2Seq(nuser, nitem, self.ntoken, args.emsize, args.nhid, args.dropout, args.nlayers).to(self.device)\n",
        "    self.text_criterion = nn.NLLLoss(ignore_index=pad_idx)  # ignore the padding when computing loss\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)\n",
        "    #optimizer = torch.optim.RMSprop(model.parameters(), lr=0.002, alpha=0.95)\n",
        "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97)\n",
        "\n",
        "  \n",
        "  ###############################################################################\n",
        "  # Training code\n",
        "  ###############################################################################\n",
        "\n",
        "  def train(self, args, data):\n",
        "    self.model.train()\n",
        "    text_loss = 0.\n",
        "    total_sample = 0\n",
        "    while True:\n",
        "      user, item, _, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "      batch_size = user.size(0)\n",
        "      user = user.to(self.device)  # (batch_size,)\n",
        "      item = item.to(self.device)\n",
        "      seq = seq.to(self.device)  # (batch_size, seq_len + 2)\n",
        "      # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "      # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "      self.optimizer.zero_grad()\n",
        "      log_word_prob = self.model(user, item, seq[:, :-1])  # (batch_size,) vs. (batch_size, seq_len + 1, ntoken)\n",
        "      loss = self.text_criterion(log_word_prob.view(-1, self.ntoken), seq[:, 1:].reshape((-1,)))\n",
        "      loss.backward()\n",
        "\n",
        "      # `clip_grad_norm` helps prevent the exploding gradient problem.\n",
        "      torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
        "      self.optimizer.step()\n",
        "\n",
        "      text_loss += batch_size * loss.item()\n",
        "      total_sample += batch_size\n",
        "\n",
        "      if data.step == data.total_step:\n",
        "        break\n",
        "    return text_loss / total_sample\n",
        "\n",
        "\n",
        "  def evaluate(self,args,data):\n",
        "    self.model.eval()\n",
        "    text_loss = 0.\n",
        "    total_sample = 0\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, _, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "        batch_size = user.size(0)\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        seq = seq.to(self.device)  # (batch_size, seq_len + 2)\n",
        "        log_word_prob = self.model(user, item, seq[:, :-1])  # (batch_size,) vs. (batch_size, seq_len + 1, ntoken)\n",
        "        loss = self.text_criterion(log_word_prob.view(-1, self.ntoken), seq[:, 1:].reshape((-1,)))\n",
        "\n",
        "        text_loss += batch_size * loss.item()\n",
        "        total_sample += batch_size\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return text_loss / total_sample\n",
        "\n",
        "\n",
        "  def generate(self, args, data):\n",
        "    self.model.eval()\n",
        "    idss_predict = []\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, _, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        inputs = seq[:, :1].to(self.device)  # (batch_size, 1)\n",
        "        hidden = None\n",
        "        hidden_c = None\n",
        "        ids = inputs\n",
        "        for idx in range(args.words):\n",
        "          # produce a word at each step\n",
        "          if idx == 0:\n",
        "            hidden = self.model.encoder(user, item)\n",
        "            hidden_c = torch.zeros_like(hidden)\n",
        "            log_word_prob, hidden, hidden_c = self.model.decoder(inputs, hidden, hidden_c)  # (batch_size, 1, ntoken)\n",
        "          else:\n",
        "            log_word_prob, hidden, hidden_c = self.model.decoder(inputs, hidden, hidden_c)  # (batch_size, 1, ntoken)\n",
        "          word_prob = log_word_prob.squeeze().exp()  # (batch_size, ntoken)\n",
        "          inputs = torch.argmax(word_prob, dim=1, keepdim=True)  # (batch_size, 1), pick the one with the largest probability\n",
        "          ids = torch.cat([ids, inputs], 1)  # (batch_size, len++)\n",
        "        ids = ids[:, 1:].tolist()  # remove bos\n",
        "        idss_predict.extend(ids)\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return idss_predict\n",
        "\n",
        "\n",
        "  def TRAIN(self,args):\n",
        "    # Loop over epochs.\n",
        "    best_val_loss = float('inf')\n",
        "    endure_count = 0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "      print(now_time() + 'epoch {}'.format(epoch))\n",
        "      train_loss = self.train(args, self.train_data)\n",
        "      print(now_time() + 'text ppl {:4.4f} on train'.format(math.exp(train_loss)))\n",
        "      val_loss = self.evaluate(args,self.val_data)\n",
        "      print(now_time() + 'text ppl {:4.4f} on validation'.format(math.exp(val_loss)))\n",
        "    #    if epoch > 10:\n",
        "    # Anneal the learning rate\n",
        "    #        scheduler.step()\n",
        "    #        print(now_time() + 'Learning rate set to {:2.8f}'.format(scheduler.get_last_lr()[0]))\n",
        "      if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        with open(self.model_path, 'wb') as f:\n",
        "            torch.save(self.model, f)\n",
        "      else:\n",
        "        endure_count += 1\n",
        "        print(now_time() + 'Endured {} time(s)'.format(endure_count))\n",
        "        if endure_count == args.endure_times:\n",
        "          print(now_time() + 'Cannot endure it anymore | Exiting from early stop')\n",
        "          break\n",
        "  \n",
        "  def TEST(self,args):\n",
        "    # Load the best saved model.\n",
        "    with open(self.model_path, 'rb') as f:\n",
        "      self.model = torch.load(f).to(self.device)\n",
        "\n",
        "    # Run on test data.\n",
        "    test_loss = self.evaluate(args,self.test_data)\n",
        "    print('=' * 89)\n",
        "    print(now_time() + 'text ppl {:4.4f} | End of training'.format(math.exp(test_loss)))\n",
        "    print(now_time() + args.model_name+' Generating text')\n",
        "    idss_predicted = self.generate(args,self.test_data)\n",
        "    tokens_test = [ids2tokens(ids[1:], self.word2idx, self.idx2word) for ids in self.test_data.seq.tolist()]\n",
        "    tokens_predict = [ids2tokens(ids, self.word2idx, self.idx2word) for ids in idss_predicted]\n",
        "    BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
        "    print(now_time() + 'BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "    BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
        "    print(now_time() + 'BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "    USR, USN = unique_sentence_percent(tokens_predict)\n",
        "    print(now_time() + 'USR {:7.4f} | USN {:7}'.format(USR, USN))\n",
        "    feature_batch = feature_detect(tokens_predict, self.feature_set)\n",
        "    DIV = feature_diversity(feature_batch)  # time-consuming\n",
        "    print(now_time() + 'DIV {:7.4f}'.format(DIV))\n",
        "    FCR = feature_coverage_ratio(feature_batch, self.feature_set)\n",
        "    print(now_time() + 'FCR {:7.4f}'.format(FCR))\n",
        "    FMR = feature_matching_ratio(feature_batch, self.test_data.feature)\n",
        "    print(now_time() + 'FMR {:7.4f}'.format(FMR))\n",
        "    text_test = [' '.join(tokens) for tokens in tokens_test]\n",
        "    text_predict = [' '.join(tokens) for tokens in tokens_predict]\n",
        "    ROUGE = rouge_score(text_test, text_predict)  # a dictionary\n",
        "    for (k, v) in ROUGE.items():\n",
        "      print(now_time() + '{} {:7.4f}'.format(k, v))\n",
        "    text_out = ''\n",
        "    for (real, fake) in zip(text_test, text_predict):\n",
        "      text_out += '{}\\n{}\\n\\n'.format(real, fake)\n",
        "    with open(self.prediction_path, 'w', encoding='utf-8') as f:\n",
        "      f.write(text_out)\n",
        "    print(now_time() + 'Generated text saved to ({})'.format(self.prediction_path))\n",
        "\n",
        "    self.ShowGenSent(text_out,3)\n",
        "\n",
        "  def ShowDataset(self,args,n=5):\n",
        "    with open(os.path.join(args.index_dir, 'validation.index'), 'r') as f:\n",
        "      valid_index = [int(x) for x in f.readline().split(' ')]\n",
        "      print('validation size: ',valid_index[0])\n",
        "      f.close()\n",
        "\n",
        "    reviews = pickle.load(open(args.data_path, 'rb'))\n",
        "    for i,review in enumerate(reviews[:n]):\n",
        "      print(i,review)\n",
        "      print('user:\\t',review['user'],'\\nitem:\\t',review['item'],'\\ntemp:\\t',review['template'],'\\nrating:\\t',review['rating'])\n",
        "      (fea, adj, tem, sco) = review['template']\n",
        "      print('fea:\\t',fea,'\\nadj:\\t', adj, '\\ntem:\\t',tem,'\\nsco:\\t', sco)\n",
        "\n",
        "  def ShowGenSent(self, gen_text,id):\n",
        "    texts = gen_text.split('\\n\\n')\n",
        "    print(id,'/',len(texts))\n",
        "    ts = texts[id].split('\\n')\n",
        "    print('GT : ',ts[0],'\\nCTX: ',ts[1],'\\nGEN: ',ts[2])\n",
        "    # print(text_test, tokens_context, text_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXDjODygEV5S"
      },
      "outputs": [],
      "source": [
        "args.model_name = 'Att2Seq'\n",
        "args.emsize = 64\n",
        "args.nhid= 512\n",
        "args.nlayers = 2\n",
        "args.lr = 1e-4\n",
        "args.clip = 5.0\n",
        "args.batch_size = 50\n",
        "args.epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "fzG64FQNDwbj",
        "outputId": "89e228c9-74c8-424c-db5a-9b5703c08a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[22-11-03 03:32:13.096343]: Att2Seq Loading data  ./Amazon/MoviesAndTV/reviews.pickle ./Amazon/MoviesAndTV/2\n",
            "[22-11-03 03:32:30.382981]: epoch 1\n",
            "[22-11-03 03:36:34.613579]: text ppl 290.4039 on train\n",
            "[22-11-03 03:36:43.832420]: text ppl 187.2061 on validation\n",
            "[22-11-03 03:36:44.120567]: epoch 2\n",
            "[22-11-03 03:40:54.796913]: text ppl 147.9127 on train\n",
            "[22-11-03 03:41:04.157024]: text ppl 128.2947 on validation\n",
            "[22-11-03 03:41:04.436905]: epoch 3\n",
            "[22-11-03 03:45:14.997641]: text ppl 112.4506 on train\n",
            "[22-11-03 03:45:24.505322]: text ppl 107.0996 on validation\n",
            "=========================================================================================\n",
            "[22-11-03 03:45:34.268138]: text ppl 106.2078 | End of training\n",
            "[22-11-03 03:45:34.268213]: Att2Seq Generating text\n",
            "[22-11-03 03:45:48.001119]: BLEU-1 14.0301\n",
            "[22-11-03 03:45:51.715689]: BLEU-4  0.7063\n",
            "[22-11-03 03:45:51.885664]: USR  0.0017 | USN      74\n",
            "[22-11-03 03:51:12.139049]: DIV  3.6314\n",
            "[22-11-03 03:51:12.163641]: FCR  0.0053\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-dda323f8054e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-4a818b3efbd1>\u001b[0m in \u001b[0;36mTEST\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mFCR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_coverage_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FCR {:7.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFCR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mFMR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_matching_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FMR {:7.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFMR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mtext_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ERS/Utils/utils.py\u001b[0m in \u001b[0;36mfeature_matching_ratio\u001b[0;34m(feature_batch, test_feature)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfea_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfea_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "a2s = Att2Seq_Model(args)\n",
        "a2s.LoadData(args)\n",
        "# nrt.ShowDataset(args)\n",
        "a2s.BuildModel(args)\n",
        "a2s.TRAIN(args)\n",
        "a2s.TEST(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvfmnneNGK7R"
      },
      "source": [
        "#PEPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMTblkJwGPiO"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AdamW\n",
        "from ESR.PEPLER.module import ContinuousPromptLearning\n",
        "from ESR.Utils.utils import DataLoader_PEPLER, Batchify_PEPLER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Auau5tvSKXFF"
      },
      "outputs": [],
      "source": [
        "class PEPLER_Model():\n",
        "  def __init__(self,args):\n",
        "    if torch.cuda.is_available():\n",
        "      if not args.cuda:\n",
        "        print(now_time() + 'WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
        "    self.device = torch.device('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "    if not os.path.exists(args.checkpoint):\n",
        "      os.makedirs(args.checkpoint)\n",
        "    self.model_path = os.path.join(args.checkpoint, 'model.pt')\n",
        "    self.prediction_path = os.path.join(args.checkpoint, args.outf)\n",
        "\n",
        "  def LoadData(self,args):\n",
        "    ###############################################################################\n",
        "    # Load data\n",
        "    ###############################################################################\n",
        "\n",
        "    print(now_time() + args.model_name+ ' Loading data')\n",
        "    bos = '<bos>'\n",
        "    self.eos = '<eos>'\n",
        "    pad = '<pad>'\n",
        "    self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token=bos, eos_token=self.eos, pad_token=pad)\n",
        "    self.corpus = DataLoader_PEPLER(args.data_path, args.index_dir, self.tokenizer, args.words)\n",
        "    self.feature_set = self.corpus.feature_set\n",
        "    self.train_data = Batchify_PEPLER(self.corpus.train, self.tokenizer, bos, self.eos, args.batch_size, shuffle=True)\n",
        "    self.val_data = Batchify_PEPLER(self.corpus.valid, self.tokenizer, bos, self.eos, args.batch_size)\n",
        "    self.test_data = Batchify_PEPLER(self.corpus.test, self.tokenizer, bos, self.eos, args.batch_size)\n",
        "\n",
        "  def BuildModel(self,args):\n",
        "    ###############################################################################\n",
        "    # Build the model\n",
        "    ###############################################################################\n",
        "\n",
        "    nuser = len(self.corpus.user_dict)\n",
        "    nitem = len(self.corpus.item_dict)\n",
        "    self.ntoken = len(self.tokenizer)\n",
        "    self.model = ContinuousPromptLearning.from_pretrained('gpt2', nuser, nitem)\n",
        "    self.model.resize_token_embeddings(self.ntoken)  # three tokens added, update embedding table\n",
        "    self.model.to(self.device)\n",
        "    self.optimizer = AdamW(self.model.parameters(), lr=args.lr)\n",
        "\n",
        "  ###############################################################################\n",
        "  # Training code\n",
        "  ###############################################################################\n",
        "\n",
        "  def train(self,args,data):\n",
        "    # Turn on training mode which enables dropout.\n",
        "    self.model.train()\n",
        "    text_loss = 0.\n",
        "    total_sample = 0\n",
        "    while True:\n",
        "      user, item, _, seq, mask = data.next_batch()  # data.step += 1\n",
        "      user = user.to(self.device)  # (batch_size,)\n",
        "      item = item.to(self.device)\n",
        "      seq = seq.to(self.device)  # (batch_size, seq_len)\n",
        "      mask = mask.to(self.device)\n",
        "      # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "      # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "      self.optimizer.zero_grad()\n",
        "      outputs = self.model(user, item, seq, mask)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      batch_size = user.size(0)\n",
        "      text_loss += batch_size * loss.item()\n",
        "      total_sample += batch_size\n",
        "\n",
        "      if data.step % args.log_interval == 0 or data.step == data.total_step:\n",
        "        cur_t_loss = text_loss / total_sample\n",
        "        print(now_time() + 'text ppl {:4.4f} | {:5d}/{:5d} batches'.format(math.exp(cur_t_loss), data.step, data.total_step))\n",
        "        text_loss = 0.\n",
        "        total_sample = 0\n",
        "      if data.step == data.total_step:\n",
        "        break\n",
        "\n",
        "\n",
        "  def evaluate(self, args, data):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    self.model.eval()\n",
        "    text_loss = 0.\n",
        "    total_sample = 0\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, _, seq, mask = data.next_batch()  # data.step += 1\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        seq = seq.to(self.device)  # (batch_size, seq_len)\n",
        "        mask = mask.to(self.device)\n",
        "        outputs = self.model(user, item, seq, mask)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        batch_size = user.size(0)\n",
        "        text_loss += batch_size * loss.item()\n",
        "        total_sample += batch_size\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return text_loss / total_sample\n",
        "\n",
        "\n",
        "  def generate(self,args,data):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    self.model.eval()\n",
        "    idss_predict = []\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, _, seq, _ = data.next_batch()  # data.step += 1\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        text = seq[:, :1].to(self.device)  # bos, (batch_size, 1)\n",
        "        for idx in range(seq.size(1)):\n",
        "          # produce a word at each step\n",
        "          outputs = self.model(user, item, text, None)\n",
        "          last_token = outputs.logits[:, -1, :]  # the last token, (batch_size, ntoken)\n",
        "          word_prob = torch.softmax(last_token, dim=-1)\n",
        "          token = torch.argmax(word_prob, dim=1, keepdim=True)  # (batch_size, 1), pick the one with the largest probability\n",
        "          text = torch.cat([text, token], 1)  # (batch_size, len++)\n",
        "        ids = text[:, 1:].tolist()  # remove bos, (batch_size, seq_len)\n",
        "        idss_predict.extend(ids)\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return idss_predict\n",
        "\n",
        "  def TRAIN(self,args):\n",
        "    # Loop over epochs.\n",
        "    best_val_loss = float('inf')\n",
        "    endure_count = 0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        print(now_time() + 'epoch {}'.format(epoch))\n",
        "        train_loss = self.train(args,self.train_data)\n",
        "        print(now_time() + 'text ppl {:4.4f} on train'.format(math.exp(train_loss)))\n",
        "        val_loss = self.evaluate(args,self.val_data)\n",
        "        print(now_time() + 'text ppl {:4.4f} on validation'.format(math.exp(val_loss)))\n",
        "    #    if epoch > 10:\n",
        "            # Anneal the learning rate\n",
        "    #        scheduler.step()\n",
        "    #        print(now_time() + 'Learning rate set to {:2.8f}'.format(scheduler.get_last_lr()[0]))\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            with open(self.model_path, 'wb') as f:\n",
        "                torch.save(self.model, f)\n",
        "        else:\n",
        "            endure_count += 1\n",
        "            print(now_time() + 'Endured {} time(s)'.format(endure_count))\n",
        "            if endure_count == args.endure_times:\n",
        "                print(now_time() + 'Cannot endure it anymore | Exiting from early stop')\n",
        "                break\n",
        "  \n",
        "  def TEST(self,args):\n",
        "    # Load the best saved model.\n",
        "    with open(self.model_path, 'rb') as f:\n",
        "      model = torch.load(f).to(self.device)\n",
        "\n",
        "\n",
        "    # Run on test data.\n",
        "    test_loss = self.evaluate(args,self.test_data)\n",
        "    print('=' * 89)\n",
        "    print(now_time() + 'text ppl {:4.4f} on test | End of training'.format(math.exp(test_loss)))\n",
        "    print(now_time() + 'Generating text')\n",
        "    idss_predicted = self.generate(self.test_data)\n",
        "    tokens_test = [ids2tokens(ids[1:], self.tokenizer, self.eos) for ids in self.test_data.seq.tolist()]\n",
        "    tokens_predict = [ids2tokens(ids, self.tokenizer, self.eos) for ids in idss_predicted]\n",
        "    BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
        "    print(now_time() + 'BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "    BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
        "    print(now_time() + 'BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "    USR, USN = unique_sentence_percent(tokens_predict)\n",
        "    print(now_time() + 'USR {:7.4f} | USN {:7}'.format(USR, USN))\n",
        "    feature_batch = feature_detect(tokens_predict, self.feature_set)\n",
        "    DIV = feature_diversity(feature_batch)  # time-consuming\n",
        "    print(now_time() + 'DIV {:7.4f}'.format(DIV))\n",
        "    FCR = feature_coverage_ratio(feature_batch, self.feature_set)\n",
        "    print(now_time() + 'FCR {:7.4f}'.format(FCR))\n",
        "    FMR = feature_matching_ratio(feature_batch, self.test_data.feature)\n",
        "    print(now_time() + 'FMR {:7.4f}'.format(FMR))\n",
        "    text_test = [' '.join(tokens) for tokens in tokens_test]\n",
        "    text_predict = [' '.join(tokens) for tokens in tokens_predict]\n",
        "    ROUGE = rouge_score(text_test, text_predict)  # a dictionary\n",
        "    for (k, v) in ROUGE.items():\n",
        "        print(now_time() + '{} {:7.4f}'.format(k, v))\n",
        "    text_out = ''\n",
        "    for (real, fake) in zip(text_test, text_predict):\n",
        "        text_out += '{}\\n{}\\n\\n'.format(real, fake)\n",
        "    with open(self.prediction_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(text_out)\n",
        "    print(now_time() + 'Generated text saved to ({})'.format(self.prediction_path))\n",
        "\n",
        "    self.ShowGenSent(text_out,3)\n",
        "\n",
        "  def ShowDataset(self,args,n=5):\n",
        "    with open(os.path.join(args.index_dir, 'validation.index'), 'r') as f:\n",
        "      valid_index = [int(x) for x in f.readline().split(' ')]\n",
        "      print('validation size: ',valid_index[0])\n",
        "      f.close()\n",
        "\n",
        "    reviews = pickle.load(open(args.data_path, 'rb'))\n",
        "    for i,review in enumerate(reviews[:n]):\n",
        "      print(i,review)\n",
        "      print('user:\\t',review['user'],'\\nitem:\\t',review['item'],'\\ntemp:\\t',review['template'],'\\nrating:\\t',review['rating'])\n",
        "      (fea, adj, tem, sco) = review['template']\n",
        "      print('fea:\\t',fea,'\\nadj:\\t', adj, '\\ntem:\\t',tem,'\\nsco:\\t', sco)\n",
        "\n",
        "  def ShowGenSent(self, gen_text,id):\n",
        "    texts = gen_text.split('\\n\\n')\n",
        "    print(id,'/',len(texts))\n",
        "    ts = texts[id].split('\\n')\n",
        "    print('GT : ',ts[0],'\\nCTX: ',ts[1],'\\nGEN: ',ts[2])\n",
        "    # print(text_test, tokens_context, text_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBSTgpUxYX2K"
      },
      "outputs": [],
      "source": [
        "args.model_name = 'PEPLER' # Continue prompt learning\n",
        "args.emsize = 64\n",
        "args.nhid= 512\n",
        "args.nlayers = 2\n",
        "args.lr = 1e-3\n",
        "args.clip = 5.0\n",
        "args.batch_size = 128\n",
        "args.words = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k63EC84cYgrF"
      },
      "outputs": [],
      "source": [
        "pepler = PEPLER_Model(args)\n",
        "pepler.LoadData(args)\n",
        "# pepler.ShowDataset(args)\n",
        "pepler.BuildModel(args)\n",
        "pepler.TRAIN(args)\n",
        "pepler.TEST(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4GIRu2WEFzrX"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}