{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TODOs:\n",
        "\n",
        "1. [ ] Config parser\n",
        "2. [ ] Using tensorflow\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gSNAlIoncurZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Datas & Codes"
      ],
      "metadata": {
        "id": "NuuEjsY4njO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hqAqZsBV_5vx",
        "outputId": "b8ab2141-dc8e-4286-9c80-d375a6347f91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eBTHQcaSrfFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Zs5UehlyUa"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/Rec/PETER/module.py ./\n",
        "!cp drive/MyDrive/Rec/PETER/bleu.py ./\n",
        "!cp drive/MyDrive/Rec/PETER/rouge.py ./\n",
        "!cp drive/MyDrive/Rec/PETER/main.py ./\n",
        "!cp drive/MyDrive/Rec/PETER/utils.py ./\n",
        "!unzip drive/MyDrive/Rec/PETER/Amazon.zip -d ./\n",
        "!unzip drive/MyDrive/Rec/PETER/Yelp.zip -d ./\n",
        "!unzip drive/MyDrive/Rec/PETER/TripAdvisor.zip -d ./\n",
        "!rm -r sample_data/\n",
        "!rm Amazon.zip\n",
        "!rm Yelp.zip\n",
        "!rm TripAdvisor.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "from module import PETER\n",
        "from utils import rouge_score, bleu_score, DataLoader, Batchify, now_time, ids2tokens, unique_sentence_percent, \\\n",
        "    root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1MO-MnRoUw0",
        "outputId": "373a5c42-7bca-4f35-a460-ca106a6d84a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Parameters"
      ],
      "metadata": {
        "id": "g7oS_iraKy3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Arg():\n",
        "  dataset_path = \"./Amazon/MoviesAndTV/\" #@param [\"./TripAdvisor/\", \"./Yelp/\", \"./Amazon/MoviesAndTV/\", \"./Amazon/ClothingShoesAndJewelry/\"] {allow-input: true}\n",
        "  index = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "  \n",
        "  data_path = os.path.join(dataset_path,\"reviews.pickle\")\n",
        "  index_dir = os.path.join(dataset_path,str(index))\n",
        "  checkpoint = os.path.join(\"./Result\",dataset_path[2:])\n",
        "  outf = \"generated.txt\" #@param {type:\"string\"}\n",
        "  \n",
        "  emsize = 512 #@param {type:\"integer\"}\n",
        "  nhead = 2 #@param {type:\"integer\"}\n",
        "  nhid = 2048 #@param {type:\"integer\"}\n",
        "  nlayers = 2 #@param {type:\"integer\"}\n",
        "  epochs = 10 #@param {type:\"slider\", min:10, max:200, step:1}\n",
        "  batch_size = 128 #@param {type:\"integer\"}\n",
        "  seed = 1111 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "  words = 15 #@param {type:\"slider\", min:12, max:20, step:1}\n",
        "  log_interval = 400 #@param {type:\"slider\", min:10, max:500, step:10}\n",
        "  vocab_size = 20000 #@param {type:\"integer\"}\n",
        "  endure_times = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "  rating_reg = 0.1 #@param {type:\"number\"}\n",
        "  context_reg = 1.0 #@param {type:\"number\"}\n",
        "  text_reg = 1.0 #@param {type:\"number\"}\n",
        "  dropout = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "  lr = 1.0 #@param {type:\"number\"}\n",
        "  clip = 1.0 #@param {type:\"number\"}\n",
        "  cuda = True #@param {type:\"boolean\"}\n",
        "  peter_mask = True #@param {type:\"boolean\"}\n",
        "  use_feature = True #@param {type:\"boolean\"}\n",
        "\n",
        "args = Arg()"
      ],
      "metadata": {
        "id": "CoGyNgw1e5lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial setup\n",
        "- Set random seed\n",
        "- Set pytorch device (/w cuda or cpu)\n",
        "- Create checkpoint folders"
      ],
      "metadata": {
        "id": "HGpwJUu_Jzkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PETER_Model():\n",
        "  def __init__(self,args):\n",
        "    # Set the random seed manually for reproducibility.\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "      if not args.cuda:\n",
        "        print(now_time() + 'WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
        "    self.device = torch.device('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "    if not os.path.exists(args.checkpoint):\n",
        "      os.makedirs(args.checkpoint)\n",
        "    self.model_path = os.path.join(args.checkpoint, 'model.pt')\n",
        "    self.prediction_path = os.path.join(args.checkpoint, args.outf)\n",
        "\n",
        "  def LoadData(self,args):\n",
        "    ###############################################################################\n",
        "    # Load data\n",
        "    ###############################################################################\n",
        "\n",
        "    print(now_time() + 'Loading data: ', args.data_path, args.index_dir)\n",
        "    self.corpus = DataLoader(args.data_path, args.index_dir, args.vocab_size)\n",
        "    self.word2idx = self.corpus.word_dict.word2idx\n",
        "    self.idx2word = self.corpus.word_dict.idx2word\n",
        "    self.feature_set = self.corpus.feature_set\n",
        "    self.train_data = Batchify(self.corpus.train, self.word2idx, args.words, args.batch_size, shuffle=True)\n",
        "    self.val_data = Batchify(self.corpus.valid, self.word2idx, args.words, args.batch_size)\n",
        "    self.test_data = Batchify(self.corpus.test, self.word2idx, args.words, args.batch_size)\n",
        "\n",
        "  def BuildModel(self,args):\n",
        "    ###############################################################################\n",
        "    # Build the model\n",
        "    ###############################################################################\n",
        "\n",
        "    if args.use_feature:\n",
        "        src_len = 2 + self.train_data.feature.size(1)  # [u, i, f]\n",
        "    else:\n",
        "        src_len = 2  # [u, i]\n",
        "    self.tgt_len = args.words + 1  # added <bos> or <eos>\n",
        "    self.ntokens = len(self.corpus.word_dict)\n",
        "    nuser = len(self.corpus.user_dict)\n",
        "    nitem = len(self.corpus.item_dict)\n",
        "    pad_idx = self.word2idx['<pad>']\n",
        "    self.model = PETER(args.peter_mask, src_len, self.tgt_len, pad_idx, nuser, nitem, self.ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(self.device)\n",
        "    self.text_criterion = nn.NLLLoss(ignore_index=pad_idx)  # ignore the padding when computing loss\n",
        "    self.rating_criterion = nn.MSELoss()\n",
        "    self.optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
        "    self.scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.25)\n",
        "\n",
        "  def predict(self, log_context_dis, topk):\n",
        "    word_prob = log_context_dis.exp()  # (batch_size, ntoken)\n",
        "    if topk == 1:\n",
        "      context = torch.argmax(word_prob, dim=1, keepdim=True)  # (batch_size, 1)\n",
        "    else:\n",
        "      context = torch.topk(word_prob, topk, 1)[1]  # (batch_size, topk)\n",
        "    return context  # (batch_size, topk)\n",
        "\n",
        "  def train(self, args, data):\n",
        "    # Turn on training mode which enables dropout.\n",
        "    self.model.train()\n",
        "    context_loss = 0.\n",
        "    text_loss = 0.\n",
        "    rating_loss = 0.\n",
        "    total_sample = 0\n",
        "    while True:\n",
        "      user, item, rating, seq, feature = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "      batch_size = user.size(0)\n",
        "      user = user.to(self.device)  # (batch_size,)\n",
        "      item = item.to(self.device)\n",
        "      rating = rating.to(self.device)\n",
        "      seq = seq.t().to(self.device)  # (tgt_len + 1, batch_size)\n",
        "      feature = feature.t().to(self.device)  # (1, batch_size)\n",
        "      if args.use_feature:\n",
        "        text = torch.cat([feature, seq[:-1]], 0)  # (src_len + tgt_len - 2, batch_size)\n",
        "      else:\n",
        "        text = seq[:-1]  # (src_len + tgt_len - 2, batch_size)\n",
        "      # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "      # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "      self.optimizer.zero_grad()\n",
        "      log_word_prob, log_context_dis, rating_p, _ = self.model(user, item, text)  # (tgt_len, batch_size, ntoken) vs. (batch_size, ntoken) vs. (batch_size,)\n",
        "      context_dis = log_context_dis.unsqueeze(0).repeat((self.tgt_len - 1, 1, 1))  # (batch_size, ntoken) -> (tgt_len - 1, batch_size, ntoken)\n",
        "      c_loss = self.text_criterion(context_dis.view(-1, self.ntokens), seq[1:-1].reshape((-1,)))\n",
        "      r_loss = self.rating_criterion(rating_p, rating)\n",
        "      t_loss = self.text_criterion(log_word_prob.view(-1, self.ntokens), seq[1:].reshape((-1,)))\n",
        "      loss = args.text_reg * t_loss + args.context_reg * c_loss + args.rating_reg * r_loss\n",
        "      loss.backward()\n",
        "\n",
        "      # `clip_grad_norm` helps prevent the exploding gradient problem.\n",
        "      torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
        "      self.optimizer.step()\n",
        "\n",
        "      context_loss += batch_size * c_loss.item()\n",
        "      text_loss += batch_size * t_loss.item()\n",
        "      rating_loss += batch_size * r_loss.item()\n",
        "      total_sample += batch_size\n",
        "\n",
        "      if data.step % args.log_interval == 0 or data.step == data.total_step:\n",
        "        cur_c_loss = context_loss / total_sample\n",
        "        cur_t_loss = text_loss / total_sample\n",
        "        cur_r_loss = rating_loss / total_sample\n",
        "        print(now_time() + 'context ppl {:4.4f} | text ppl {:4.4f} | rating loss {:4.4f} | {:5d}/{:5d} batches'.format(\n",
        "              math.exp(cur_c_loss), math.exp(cur_t_loss), cur_r_loss, data.step, data.total_step))\n",
        "        context_loss = 0.\n",
        "        text_loss = 0.\n",
        "        rating_loss = 0.\n",
        "        total_sample = 0\n",
        "      if data.step == data.total_step:\n",
        "        break\n",
        "\n",
        "  def evaluate(self,args, data):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    self.model.eval()\n",
        "    context_loss = 0.\n",
        "    text_loss = 0.\n",
        "    rating_loss = 0.\n",
        "    total_sample = 0\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, rating, seq, feature = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "        batch_size = user.size(0)\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        rating = rating.to(self.device)\n",
        "        seq = seq.t().to(self.device)  # (tgt_len + 1, batch_size)\n",
        "        feature = feature.t().to(self.device)  # (1, batch_size)\n",
        "        if args.use_feature:\n",
        "          text = torch.cat([feature, seq[:-1]], 0)  # (src_len + tgt_len - 2, batch_size)\n",
        "        else:\n",
        "          text = seq[:-1]  # (src_len + tgt_len - 2, batch_size)\n",
        "        log_word_prob, log_context_dis, rating_p, _ = self.model(user, item, text)  # (tgt_len, batch_size, ntoken) vs. (batch_size, ntoken) vs. (batch_size,)\n",
        "        context_dis = log_context_dis.unsqueeze(0).repeat((self.tgt_len - 1, 1, 1))  # (batch_size, ntoken) -> (tgt_len - 1, batch_size, ntoken)\n",
        "        c_loss = self.text_criterion(context_dis.view(-1, self.ntokens), seq[1:-1].reshape((-1,)))\n",
        "        r_loss = self.rating_criterion(rating_p, rating)\n",
        "        t_loss = self.text_criterion(log_word_prob.view(-1, self.ntokens), seq[1:].reshape((-1,)))\n",
        "\n",
        "        context_loss += batch_size * c_loss.item()\n",
        "        text_loss += batch_size * t_loss.item()\n",
        "        rating_loss += batch_size * r_loss.item()\n",
        "        total_sample += batch_size\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return context_loss / total_sample, text_loss / total_sample, rating_loss / total_sample\n",
        "\n",
        "  def generate(self,args,data):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    self.model.eval()\n",
        "    idss_predict = []\n",
        "    context_predict = []\n",
        "    rating_predict = []\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, rating, seq, feature = data.next_batch()\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        bos = seq[:, 0].unsqueeze(0).to(self.device)  # (1, batch_size)\n",
        "        feature = feature.t().to(self.device)  # (1, batch_size)\n",
        "        if args.use_feature:\n",
        "          text = torch.cat([feature, bos], 0)  # (src_len - 1, batch_size)\n",
        "        else:\n",
        "          text = bos  # (src_len - 1, batch_size)\n",
        "        start_idx = text.size(0)\n",
        "        for idx in range(args.words):\n",
        "          # produce a word at each step\n",
        "          if idx == 0:\n",
        "            log_word_prob, log_context_dis, rating_p, _ = self.model(user, item, text, False)  # (batch_size, ntoken) vs. (batch_size, ntoken) vs. (batch_size,)\n",
        "            rating_predict.extend(rating_p.tolist())\n",
        "            context = self.predict(log_context_dis, topk=args.words)  # (batch_size, words)\n",
        "            context_predict.extend(context.tolist())\n",
        "          else:\n",
        "            log_word_prob, _, _, _ = self.model(user, item, text, False, False, False)  # (batch_size, ntoken)\n",
        "          word_prob = log_word_prob.exp()  # (batch_size, ntoken)\n",
        "          word_idx = torch.argmax(word_prob, dim=1)  # (batch_size,), pick the one with the largest probability\n",
        "          text = torch.cat([text, word_idx.unsqueeze(0)], 0)  # (len++, batch_size)\n",
        "        ids = text[start_idx:].t().tolist()  # (batch_size, seq_len)\n",
        "        idss_predict.extend(ids)\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "\n",
        "    # rating\n",
        "    predicted_rating = [(r, p) for (r, p) in zip(data.rating.tolist(), rating_predict)]\n",
        "    RMSE = root_mean_square_error(predicted_rating, self.corpus.max_rating, self.corpus.min_rating)\n",
        "    print(now_time() + 'RMSE {:7.4f}'.format(RMSE))\n",
        "    MAE = mean_absolute_error(predicted_rating, self.corpus.max_rating, self.corpus.min_rating)\n",
        "    print(now_time() + 'MAE {:7.4f}'.format(MAE))\n",
        "    # text\n",
        "    tokens_test = [ids2tokens(ids[1:], self.word2idx, self.idx2word) for ids in data.seq.tolist()]\n",
        "    tokens_predict = [ids2tokens(ids, self.word2idx, self.idx2word) for ids in idss_predict]\n",
        "    BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
        "    print(now_time() + 'BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "    BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
        "    print(now_time() + 'BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "    USR, USN = unique_sentence_percent(tokens_predict)\n",
        "    print(now_time() + 'USR {:7.4f} | USN {:7}'.format(USR, USN))\n",
        "    feature_batch = feature_detect(tokens_predict, self.feature_set)\n",
        "    DIV = feature_diversity(feature_batch)  # time-consuming\n",
        "    print(now_time() + 'DIV {:7.4f}'.format(DIV))\n",
        "    FCR = feature_coverage_ratio(feature_batch, self.feature_set)\n",
        "    print(now_time() + 'FCR {:7.4f}'.format(FCR))\n",
        "    feature_test = [self.idx2word[i] for i in data.feature.squeeze(1).tolist()]  # ids to words\n",
        "    FMR = feature_matching_ratio(feature_batch, feature_test)\n",
        "    print(now_time() + 'FMR {:7.4f}'.format(FMR))\n",
        "    text_test = [' '.join(tokens) for tokens in tokens_test]\n",
        "    text_predict = [' '.join(tokens) for tokens in tokens_predict]\n",
        "    tokens_context = [' '.join([idx2word[i] for i in ids]) for ids in context_predict]\n",
        "    ROUGE = rouge_score(text_test, text_predict)  # a dictionary\n",
        "    for (k, v) in ROUGE.items():\n",
        "      print(now_time() + '{} {:7.4f}'.format(k, v))\n",
        "    text_out = ''\n",
        "    for (real, ctx, fake) in zip(text_test, tokens_context, text_predict):\n",
        "      text_out += '{}\\n{}\\n{}\\n\\n'.format(real, ctx, fake)\n",
        "    return text_out\n",
        "\n",
        "  def TRAIN(self,args):\n",
        "      \n",
        "    # Loop over epochs.\n",
        "    best_val_loss = float('inf')\n",
        "    endure_count = 0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "      print(now_time() + 'epoch {}'.format(epoch))\n",
        "      self.train(args,self.train_data)\n",
        "      val_c_loss, val_t_loss, val_r_loss = self.evaluate(args,self.val_data)\n",
        "      if args.rating_reg == 0:\n",
        "        val_loss = val_t_loss\n",
        "      else:\n",
        "        val_loss = val_t_loss + val_r_loss\n",
        "      print(now_time() + 'context ppl {:4.4f} | text ppl {:4.4f} | rating loss {:4.4f} | valid loss {:4.4f} on validation'.format(\n",
        "        math.exp(val_c_loss), math.exp(val_t_loss), val_r_loss, val_loss))\n",
        "      # Save the model if the validation loss is the best we've seen so far.\n",
        "      if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        with open(self.model_path, 'wb') as f:\n",
        "          torch.save(self.model, f)\n",
        "      else:\n",
        "        endure_count += 1\n",
        "        print(now_time() + 'Endured {} time(s)'.format(endure_count))\n",
        "        if endure_count == args.endure_times:\n",
        "            print(now_time() + 'Cannot endure it anymore | Exiting from early stop')\n",
        "            break\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        self.scheduler.step()\n",
        "        print(now_time() + 'Learning rate set to {:2.8f}'.format(self.scheduler.get_last_lr()[0]))\n",
        "\n",
        "  def TEST(self,args):\n",
        "    # Load the best saved model.\n",
        "    with open(self.model_path, 'rb') as f:\n",
        "        self.model = torch.load(f).to(self.device)\n",
        "\n",
        "    # Run on test data.\n",
        "    test_c_loss, test_t_loss, test_r_loss = self.evaluate(args,self.test_data)\n",
        "    print('=' * 89)\n",
        "    print(now_time() + 'context ppl {:4.4f} | text ppl {:4.4f} | rating loss {:4.4f} on test | End of training'.format(\n",
        "        math.exp(test_c_loss), math.exp(test_t_loss), test_r_loss))\n",
        "\n",
        "    print(now_time() + 'Generating text')\n",
        "    text_o = self.generate(args,self.test_data)\n",
        "    with open(self.prediction_path, 'w', encoding='utf-8') as f:\n",
        "      f.write(text_o)\n",
        "    print(now_time() + 'Generated text saved to ({})'.format(self.prediction_path))\n",
        "\n",
        "    self.ShowGenSent(text_o,3)\n",
        "  \n",
        "  def ShowDataset(self,args,n=5):\n",
        "    with open(os.path.join(args.index_dir, 'validation.index'), 'r') as f:\n",
        "      valid_index = [int(x) for x in f.readline().split(' ')]\n",
        "      print('validation size: ',valid_index[0])\n",
        "      f.close()\n",
        "\n",
        "    reviews = pickle.load(open(args.data_path, 'rb'))\n",
        "    for i,review in enumerate(reviews[:n]):\n",
        "      print(i,review)\n",
        "      print('user:\\t',review['user'],'\\nitem:\\t',review['item'],'\\ntemp:\\t',review['template'],'\\nrating:\\t',review['rating'])\n",
        "      (fea, adj, tem, sco) = review['template']\n",
        "      print('fea:\\t',fea,'\\nadj:\\t', adj, '\\ntem:\\t',tem,'\\nsco:\\t', sco)\n",
        "\n",
        "  def ShowGenSent(self, gen_text,id):\n",
        "    texts = gen_text.split('\\n\\n')\n",
        "    print(id,'/',len(texts))\n",
        "    ts = texts[id].split('\\n')\n",
        "    print('GT : ',ts[0],'\\nCTX: ',ts[1],'\\nGEN: ',ts[2])\n",
        "    # print(text_test, tokens_context, text_predict)\n"
      ],
      "metadata": {
        "id": "CkW033U0PXTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peter = PETER_Model(args)\n",
        "peter.LoadData(args)\n",
        "peter.ShowDataset(args)\n",
        "peter.BuildModel(args)\n",
        "peter.TRAIN(args)\n",
        "peter.TEST(args)"
      ],
      "metadata": {
        "id": "jR8VHuk9bNZ7",
        "outputId": "5394dbae-1fdc-4671-ac69-48e25d0e11df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-11-02 22:23:39.002609]: Loading data:  ./Amazon/MoviesAndTV/reviews.pickle ./Amazon/MoviesAndTV/2\n",
            "[2022-11-02 22:23:52.885782]: epoch 1\n",
            "[2022-11-02 22:24:20.819452]: context ppl 47382.5616 | text ppl 45035.2354 | rating loss 13.6080 |   400/ 2762 batches\n",
            "[2022-11-02 22:24:49.352403]: context ppl 47463.1084 | text ppl 44949.0962 | rating loss 13.4653 |   800/ 2762 batches\n",
            "[2022-11-02 22:25:18.522203]: context ppl 47495.7530 | text ppl 45202.9664 | rating loss 12.7859 |  1200/ 2762 batches\n",
            "[2022-11-02 22:25:48.196921]: context ppl 47747.5363 | text ppl 44977.4790 | rating loss 11.4419 |  1600/ 2762 batches\n",
            "[2022-11-02 22:26:18.378987]: context ppl 47365.7255 | text ppl 44894.6036 | rating loss 11.1933 |  2000/ 2762 batches\n",
            "[2022-11-02 22:26:48.807786]: context ppl 47359.3785 | text ppl 45218.6345 | rating loss 10.8277 |  2400/ 2762 batches\n",
            "[2022-11-02 22:27:16.126388]: context ppl 47469.5643 | text ppl 45208.3515 | rating loss 11.1706 |  2762/ 2762 batches\n",
            "[2022-11-02 22:27:25.490293]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:27:25.906894]: epoch 2\n",
            "[2022-11-02 22:27:56.676636]: context ppl 47421.3536 | text ppl 45084.6954 | rating loss 12.0468 |   400/ 2762 batches\n",
            "[2022-11-02 22:28:27.171469]: context ppl 47421.7220 | text ppl 45070.9275 | rating loss 12.0622 |   800/ 2762 batches\n",
            "[2022-11-02 22:28:57.570600]: context ppl 47465.9774 | text ppl 45049.0325 | rating loss 12.1150 |  1200/ 2762 batches\n",
            "[2022-11-02 22:29:27.911111]: context ppl 47368.8186 | text ppl 45021.7694 | rating loss 12.0916 |  1600/ 2762 batches\n",
            "[2022-11-02 22:29:58.261002]: context ppl 47362.4307 | text ppl 44967.6745 | rating loss 12.1084 |  2000/ 2762 batches\n",
            "[2022-11-02 22:30:28.597492]: context ppl 47448.2521 | text ppl 44886.5517 | rating loss 12.0852 |  2400/ 2762 batches\n",
            "[2022-11-02 22:30:56.002090]: context ppl 47527.7296 | text ppl 45070.1969 | rating loss 12.0459 |  2762/ 2762 batches\n",
            "[2022-11-02 22:31:05.387628]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:31:05.387790]: Endured 1 time(s)\n",
            "[2022-11-02 22:31:05.389585]: Learning rate set to 0.00097656\n",
            "[2022-11-02 22:31:05.389633]: epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-11-02 22:31:36.037226]: context ppl 47326.8571 | text ppl 44997.0674 | rating loss 12.0830 |   400/ 2762 batches\n",
            "[2022-11-02 22:32:06.462627]: context ppl 47621.6900 | text ppl 45055.1683 | rating loss 12.0415 |   800/ 2762 batches\n",
            "[2022-11-02 22:32:36.920785]: context ppl 47473.9108 | text ppl 45055.4206 | rating loss 12.1151 |  1200/ 2762 batches\n",
            "[2022-11-02 22:33:07.385099]: context ppl 47416.6424 | text ppl 44933.5833 | rating loss 12.1282 |  1600/ 2762 batches\n",
            "[2022-11-02 22:33:37.882602]: context ppl 47468.7321 | text ppl 44947.4373 | rating loss 12.0592 |  2000/ 2762 batches\n",
            "[2022-11-02 22:34:08.344511]: context ppl 47387.7166 | text ppl 44980.1070 | rating loss 12.0610 |  2400/ 2762 batches\n",
            "[2022-11-02 22:34:35.854143]: context ppl 47375.6929 | text ppl 45041.3135 | rating loss 12.1004 |  2762/ 2762 batches\n",
            "[2022-11-02 22:34:45.271417]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:34:45.271567]: Endured 2 time(s)\n",
            "[2022-11-02 22:34:45.271664]: Learning rate set to 0.00024414\n",
            "[2022-11-02 22:34:45.271701]: epoch 4\n",
            "[2022-11-02 22:35:15.972749]: context ppl 47372.4072 | text ppl 45062.1005 | rating loss 12.1201 |   400/ 2762 batches\n",
            "[2022-11-02 22:35:46.434404]: context ppl 47296.8967 | text ppl 44938.4174 | rating loss 12.0703 |   800/ 2762 batches\n",
            "[2022-11-02 22:36:16.859883]: context ppl 47468.6570 | text ppl 44966.6207 | rating loss 12.1090 |  1200/ 2762 batches\n",
            "[2022-11-02 22:36:47.299537]: context ppl 47590.8548 | text ppl 45000.7884 | rating loss 12.0816 |  1600/ 2762 batches\n",
            "[2022-11-02 22:37:17.739491]: context ppl 47628.8738 | text ppl 45097.8464 | rating loss 12.0564 |  2000/ 2762 batches\n",
            "[2022-11-02 22:37:48.160075]: context ppl 47446.1445 | text ppl 45077.6076 | rating loss 12.0975 |  2400/ 2762 batches\n",
            "[2022-11-02 22:38:15.612738]: context ppl 47447.2691 | text ppl 44948.2905 | rating loss 12.0112 |  2762/ 2762 batches\n",
            "[2022-11-02 22:38:25.020440]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:38:25.020584]: Endured 3 time(s)\n",
            "[2022-11-02 22:38:25.020662]: Learning rate set to 0.00006104\n",
            "[2022-11-02 22:38:25.020690]: epoch 5\n",
            "[2022-11-02 22:38:55.643685]: context ppl 47451.8848 | text ppl 45162.9095 | rating loss 12.1195 |   400/ 2762 batches\n",
            "[2022-11-02 22:39:26.037769]: context ppl 47265.2063 | text ppl 44852.2777 | rating loss 12.0615 |   800/ 2762 batches\n",
            "[2022-11-02 22:39:56.450372]: context ppl 47502.7502 | text ppl 45145.6708 | rating loss 12.0800 |  1200/ 2762 batches\n",
            "[2022-11-02 22:40:26.852756]: context ppl 47497.7773 | text ppl 44915.9619 | rating loss 12.1267 |  1600/ 2762 batches\n",
            "[2022-11-02 22:40:57.286079]: context ppl 47490.7240 | text ppl 44990.6292 | rating loss 12.0848 |  2000/ 2762 batches\n",
            "[2022-11-02 22:41:27.761743]: context ppl 47401.9525 | text ppl 44973.4562 | rating loss 12.0730 |  2400/ 2762 batches\n",
            "[2022-11-02 22:41:55.264758]: context ppl 47410.0121 | text ppl 45107.1160 | rating loss 12.0599 |  2762/ 2762 batches\n",
            "[2022-11-02 22:42:04.706349]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:42:04.706483]: Endured 4 time(s)\n",
            "[2022-11-02 22:42:04.706575]: Learning rate set to 0.00001526\n",
            "[2022-11-02 22:42:04.706611]: epoch 6\n",
            "[2022-11-02 22:42:35.440266]: context ppl 47562.2786 | text ppl 44954.6498 | rating loss 12.0655 |   400/ 2762 batches\n",
            "[2022-11-02 22:43:05.943080]: context ppl 47498.9218 | text ppl 45106.4804 | rating loss 12.0732 |   800/ 2762 batches\n",
            "[2022-11-02 22:43:36.256477]: context ppl 47608.5823 | text ppl 45062.5175 | rating loss 12.1332 |  1200/ 2762 batches\n",
            "[2022-11-02 22:44:06.812121]: context ppl 47390.6365 | text ppl 44992.5954 | rating loss 12.0894 |  1600/ 2762 batches\n",
            "[2022-11-02 22:44:37.202843]: context ppl 47444.0166 | text ppl 45048.1502 | rating loss 12.0789 |  2000/ 2762 batches\n",
            "[2022-11-02 22:45:07.626314]: context ppl 47499.8290 | text ppl 45057.6437 | rating loss 12.0795 |  2400/ 2762 batches\n",
            "[2022-11-02 22:45:35.205022]: context ppl 47244.5593 | text ppl 44962.2615 | rating loss 12.0972 |  2762/ 2762 batches\n",
            "[2022-11-02 22:45:44.659649]: context ppl 47739.8632 | text ppl 44897.7224 | rating loss 11.9134 | valid loss 22.6256 on validation\n",
            "[2022-11-02 22:45:44.659797]: Endured 5 time(s)\n",
            "[2022-11-02 22:45:44.659825]: Cannot endure it anymore | Exiting from early stop\n",
            "=========================================================================================\n",
            "[2022-11-02 22:45:54.242911]: context ppl 47929.8646 | text ppl 45132.3390 | rating loss 11.9484 on test | End of training\n",
            "[2022-11-02 22:45:54.242980]: Generating text\n",
            "[2022-11-02 22:46:35.005767]: RMSE  3.1772\n",
            "[2022-11-02 22:46:35.016534]: MAE  2.9480\n",
            "[2022-11-02 22:46:36.781412]: BLEU-1  0.0308\n",
            "[2022-11-02 22:46:41.396893]: BLEU-4  0.0000\n",
            "[2022-11-02 22:51:32.281715]: USR  0.7499 | USN   33132\n",
            "[2022-11-02 22:55:54.337686]: DIV  0.1351\n",
            "[2022-11-02 22:55:56.451318]: FCR  0.3521\n",
            "[2022-11-02 22:55:56.463233]: FMR  0.0006\n",
            "[2022-11-02 22:56:03.000991]: rouge_1/f_score  0.0349\n",
            "[2022-11-02 22:56:03.001148]: rouge_1/r_score  0.0431\n",
            "[2022-11-02 22:56:03.001190]: rouge_1/p_score  0.0314\n",
            "[2022-11-02 22:56:03.001224]: rouge_2/f_score  0.0000\n",
            "[2022-11-02 22:56:03.001257]: rouge_2/r_score  0.0000\n",
            "[2022-11-02 22:56:03.001288]: rouge_2/p_score  0.0000\n",
            "[2022-11-02 22:56:03.001321]: rouge_l/f_score  0.0318\n",
            "[2022-11-02 22:56:03.001353]: rouge_l/r_score  0.0413\n",
            "[2022-11-02 22:56:03.001386]: rouge_l/p_score  0.0306\n",
            "[2022-11-02 22:56:03.099830]: Generated text saved to (./Result/Amazon/MoviesAndTV/generated.txt)\n",
            "3 / 44180\n",
            "GT :  do n't waste your time on this loser . \n",
            "CTX:  enslaved genius publicly concentrates rereleased gump inexperienced extras saul never trio 29 expressive superbit winners \n",
            "GEN:  blackest communication clint larger details snob goblet crowd inarritu favourites hysterically belle footloose satisfies demeaning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NRT"
      ],
      "metadata": {
        "id": "4GIRu2WEFzrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from module import NRT\n",
        "from utils import DataLoader, Batchify"
      ],
      "metadata": {
        "id": "fvHmLAK-F4Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NRT Class"
      ],
      "metadata": {
        "id": "tCV7X-RjfWfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NRT_Model():\n",
        "  def __init__(self,args):\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "      if not args.cuda:\n",
        "        print(now_time() + 'WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
        "    self.device = torch.device('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "    if not os.path.exists(args.checkpoint):\n",
        "      os.makedirs(args.checkpoint)\n",
        "    self.model_path = os.path.join(args.checkpoint, 'model.pt')\n",
        "    self.prediction_path = os.path.join(args.checkpoint, args.outf)\n",
        "    self.NAME = 'NRT'\n",
        "\n",
        "  def LoadData(self,args):\n",
        "    ###############################################################################\n",
        "    # Load data\n",
        "    ###############################################################################\n",
        "\n",
        "    print(now_time() + self.NAME,'Loading data: ', args.data_path, args.index_dir)\n",
        "    self.corpus = DataLoader(args.data_path, args.index_dir, args.vocab_size)\n",
        "    self.word2idx = self.corpus.word_dict.word2idx\n",
        "    self.idx2word = self.corpus.word_dict.idx2word\n",
        "    self.feature_set = self.corpus.feature_set\n",
        "    self.train_data = Batchify(self.corpus.train, self.word2idx, args.words, args.batch_size, shuffle=True)\n",
        "    self.val_data = Batchify(self.corpus.valid, self.word2idx, args.words, args.batch_size)\n",
        "    self.test_data = Batchify(self.corpus.test, self.word2idx, args.words, args.batch_size)\n",
        "\n",
        "  def BuildModel(self,args):\n",
        "    ###############################################################################\n",
        "    # Build the model\n",
        "    ###############################################################################\n",
        "\n",
        "    nuser = len(self.corpus.user_dict)\n",
        "    nitem = len(self.corpus.item_dict)\n",
        "    self.ntoken = len(self.corpus.word_dict)\n",
        "    pad_idx = self.word2idx['<pad>']\n",
        "    self.model = NRT(nuser, nitem, ntoken, args.emsize, args.nhid, args.nlayers, self.corpus.max_rating, self.corpus.min_rating).to(self.device)\n",
        "    self.text_criterion = nn.NLLLoss(ignore_index=pad_idx)  # ignore the padding when computing loss\n",
        "    self.rating_criterion = nn.MSELoss()\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)\n",
        "    #optimizer = torch.optim.Adadelta(model.parameters())  # lr is optional to Adadelta\n",
        "\n",
        "  \n",
        "  def train(self, atgs, data):\n",
        "    self.model.train()\n",
        "    text_loss = 0.\n",
        "    rating_loss = 0.\n",
        "    total_sample = 0\n",
        "    while True:\n",
        "      user, item, rating, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "      batch_size = user.size(0)\n",
        "      user = user.to(self.device)  # (batch_size,)\n",
        "      item = item.to(self.device)\n",
        "      rating = rating.to(self.device)\n",
        "      seq = seq.to(self.device)  # (batch_size, seq_len + 2)\n",
        "      # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "      # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "      self.optimizer.zero_grad()\n",
        "      rating_p, log_word_prob = self.model(user, item, seq[:, :-1])  # (batch_size,) vs. (batch_size, seq_len + 1, ntoken)\n",
        "      r_loss = rating_criterion(rating_p, rating)\n",
        "      t_loss = text_criterion(log_word_prob.view(-1, self.ntoken), seq[:, 1:].reshape((-1,)))\n",
        "      l2_loss = torch.cat([x.view(-1) for x in self.model.parameters()]).pow(2.).sum()\n",
        "      loss = args.text_reg * t_loss + args.rating_reg * r_loss + args.l2_reg * l2_loss\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      text_loss += batch_size * t_loss.item()\n",
        "      rating_loss += batch_size * r_loss.item()\n",
        "      total_sample += batch_size\n",
        "\n",
        "      if data.step == data.total_step:\n",
        "        break\n",
        "    return text_loss / total_sample, rating_loss / total_sample\n",
        "\n",
        "\n",
        "  def evaluate(self, args, data):\n",
        "    self.model.eval()\n",
        "    text_loss = 0.\n",
        "    rating_loss = 0.\n",
        "    total_sample = 0\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, rating, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "        batch_size = user.size(0)\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        rating = rating.to(self.device)\n",
        "        seq = seq.to(self.device)  # (batch_size, seq_len + 2)\n",
        "        rating_p, log_word_prob = self.model(user, item, seq[:, :-1])  # (batch_size,) vs. (batch_size, seq_len + 1, ntoken)\n",
        "        r_loss = self.rating_criterion(rating_p, rating)\n",
        "        t_loss = self.text_criterion(log_word_prob.view(-1, self.ntoken), seq[:, 1:].reshape((-1,)))\n",
        "\n",
        "        text_loss += batch_size * t_loss.item()\n",
        "        rating_loss += batch_size * r_loss.item()\n",
        "        total_sample += batch_size\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return text_loss / total_sample, rating_loss / total_sample\n",
        "\n",
        "\n",
        "  def generate(self, args, data):\n",
        "    self.model.eval()\n",
        "    idss_predict = []\n",
        "    rating_predict = []\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "        user, item, _, seq = data.next_batch()  # (batch_size, seq_len), data.step += 1\n",
        "        user = user.to(self.device)  # (batch_size,)\n",
        "        item = item.to(self.device)\n",
        "        inputs = seq[:, :1].to(self.device)  # (batch_size, 1)\n",
        "        hidden = None\n",
        "        ids = inputs\n",
        "        for idx in range(args.words):\n",
        "          # produce a word at each step\n",
        "          if idx == 0:\n",
        "            rating_p, hidden = self.model.encoder(user, item)\n",
        "            rating_predict.extend(rating_p.tolist())\n",
        "            log_word_prob, hidden = self.model.decoder(inputs, hidden)  # (batch_size, 1, ntoken)\n",
        "          else:\n",
        "            log_word_prob, hidden = self.model.decoder(inputs, hidden)  # (batch_size, 1, ntoken)\n",
        "          word_prob = log_word_prob.squeeze().exp()  # (batch_size, ntoken)\n",
        "          inputs = torch.argmax(word_prob, dim=1, keepdim=True)  # (batch_size, 1), pick the one with the largest probability\n",
        "          ids = torch.cat([ids, inputs], 1)  # (batch_size, len++)\n",
        "        ids = ids[:, 1:].tolist()  # remove bos\n",
        "        idss_predict.extend(ids)\n",
        "\n",
        "        if data.step == data.total_step:\n",
        "          break\n",
        "    return idss_predict, rating_predict\n",
        "\n",
        "\n",
        "  def TRAIN(self,args):\n",
        "    # Loop over epochs.\n",
        "    best_val_loss = float('inf')\n",
        "    endure_count = 0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        print(now_time() + 'epoch {}'.format(epoch))\n",
        "        train_t_loss, train_r_loss = self.train(args,train_data)\n",
        "        print(now_time() + 'text ppl {:4.4f} | rating loss {:4.4f} | total loss {:4.4f} on train'.format(\n",
        "            math.exp(train_t_loss), train_r_loss, train_t_loss + train_r_loss))\n",
        "        val_t_loss, val_r_loss = self.evaluate(args,val_data)\n",
        "        val_loss = val_t_loss + val_r_loss\n",
        "        print(now_time() + 'text ppl {:4.4f} | rating loss {:4.4f} | total loss {:4.4f} on validation'.format(\n",
        "            math.exp(val_t_loss), val_r_loss, val_loss))\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            with open(self.model_path, 'wb') as f:\n",
        "                torch.save(self.model, f)\n",
        "        else:\n",
        "            endure_count += 1\n",
        "            print(now_time() + 'Endured {} time(s)'.format(endure_count))\n",
        "            if endure_count == args.endure_times:\n",
        "                print(now_time() + 'Cannot endure it anymore | Exiting from early stop')\n",
        "                break\n",
        "\n",
        "  def TEST(self,args):\n",
        "    # Load the best saved model.\n",
        "    with open(self.model_path, 'rb') as f:\n",
        "        self.model = torch.load(f).to(self.device)\n",
        "\n",
        "    # Run on test data.\n",
        "    test_t_loss, test_r_loss = self.evaluate(args,self.test_data)\n",
        "    print('=' * 89)\n",
        "    print(now_time() + 'text ppl {:4.4f} | rating loss {:4.4f} | total loss {:4.4f} on test | End of training'.format(\n",
        "            math.exp(test_t_loss), test_r_loss, test_t_loss + test_r_loss))\n",
        "    print(now_time() + 'Generating text')\n",
        "    idss_predicted, rating_predicted = self.generate(args,self.test_data)\n",
        "    # rating\n",
        "    predicted_rating = [(r, p) for (r, p) in zip(test_data.rating.tolist(), rating_predicted)]\n",
        "    RMSE = root_mean_square_error(predicted_rating, self.corpus.max_rating, self.corpus.min_rating)\n",
        "    print(now_time() + 'RMSE {:7.4f}'.format(RMSE))\n",
        "    MAE = mean_absolute_error(predicted_rating, self.corpus.max_rating, self.corpus.min_rating)\n",
        "    print(now_time() + 'MAE {:7.4f}'.format(MAE))\n",
        "    # text\n",
        "    tokens_test = [ids2tokens(ids[1:], self.word2idx, self.idx2word) for ids in test_data.seq.tolist()]\n",
        "    tokens_predict = [ids2tokens(ids, self.word2idx, self.idx2word) for ids in idss_predicted]\n",
        "    BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
        "    print(now_time() + 'BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "    BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
        "    print(now_time() + 'BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "    USR, USN = unique_sentence_percent(tokens_predict)\n",
        "    print(now_time() + 'USR {:7.4f} | USN {:7}'.format(USR, USN))\n",
        "    feature_batch = feature_detect(tokens_predict, feature_set)\n",
        "    DIV = feature_diversity(feature_batch)  # time-consuming\n",
        "    print(now_time() + 'DIV {:7.4f}'.format(DIV))\n",
        "    FCR = feature_coverage_ratio(feature_batch, feature_set)\n",
        "    print(now_time() + 'FCR {:7.4f}'.format(FCR))\n",
        "    FMR = feature_matching_ratio(feature_batch, test_data.feature)\n",
        "    print(now_time() + 'FMR {:7.4f}'.format(FMR))\n",
        "    text_test = [' '.join(tokens) for tokens in tokens_test]\n",
        "    text_predict = [' '.join(tokens) for tokens in tokens_predict]\n",
        "    ROUGE = rouge_score(text_test, text_predict)  # a dictionary\n",
        "    for (k, v) in ROUGE.items():\n",
        "        print(now_time() + '{} {:7.4f}'.format(k, v))\n",
        "    text_out = ''\n",
        "    for (real, fake) in zip(text_test, text_predict):\n",
        "        text_out += '{}\\n{}\\n\\n'.format(real, fake)\n",
        "    with open(self.prediction_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(text_out)\n",
        "    print(now_time() + 'Generated text saved to ({})'.format(self.prediction_path))\n",
        "\n",
        "    self.ShowGenSent(text_o,3)\n",
        "\n",
        "  def ShowGenSent(self, gen_text,id):\n",
        "    texts = gen_text.split('\\n\\n')\n",
        "    print(id,'/',len(texts))\n",
        "    ts = texts[id].split('\\n')\n",
        "    print('GT : ',ts[0],'\\nCTX: ',ts[1],'\\nGEN: ',ts[2])\n",
        "    # print(text_test, tokens_context, text_predict)\n"
      ],
      "metadata": {
        "id": "caRjZ8AhfZKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nrt = NRT_Model(args)\n",
        "nrt.LoadData(args)\n",
        "nrt.BuildModel(args)\n",
        "nrt.TRAIN(args)\n",
        "nrt.TEST(args)"
      ],
      "metadata": {
        "id": "p-bqsR90kBtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}